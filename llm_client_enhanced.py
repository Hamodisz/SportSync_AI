# -*- coding: utf-8 -*-
"""
SportSync AI - Enhanced Multi-Provider LLM Client
=================================================
Ù†Ø¸Ø§Ù… Ù…ÙˆØ­Ø¯ ÙˆÙ…Ø­Ø³Ù‘Ù† Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ø²ÙˆØ¯ÙŠ LLM Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ÙŠÙ†

Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ù…Ø­Ø¯Ø«Ø©:
âœ… Ø¯Ø¹Ù… 6+ Ù…Ø²ÙˆØ¯ÙŠ LLM (OpenAI, Groq, Anthropic, Google, OpenRouter, DeepSeek)
âœ… Ù†Ø¸Ø§Ù… fallback Ø°ÙƒÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ ÙˆØ§Ø¶Ø­Ø©
âœ… ØªÙƒÙ„ÙØ© Ù…Ø­Ø³Ù‘Ù†Ø© (ÙŠØ¨Ø¯Ø£ Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø±Ø®Øµ)
âœ… ÙƒØ§Ø´ Ø°ÙƒÙŠ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ
âœ… Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø±Ø¤ÙŠØ© SportSync AI Ø§Ù„ÙƒØ§Ù…Ù„Ø©

Ø§Ù„Ø±Ø¤ÙŠØ©: "Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ© Ù„ÙƒÙ„ Ø´Ø®Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø´Ø®ØµÙŠØªÙ‡ ÙˆØ§Ø­ØªÙŠØ§Ø¬Ø§ØªÙ‡"
"""

from __future__ import annotations

import os
import re
import time
import json
import random
import threading
import logging
from typing import List, Dict, Optional, Any, Tuple, Generator
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# =========================
# Logging Configuration
# =========================
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | %(name)s | %(message)s"
)
logger = logging.getLogger("SportSync.LLM")

# =========================
# Provider Configuration
# =========================
class Provider(Enum):
    """Ù…Ø²ÙˆØ¯Ùˆ LLM Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…ÙˆÙ†"""
    OPENAI = "openai"
    GROQ = "groq"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    OPENROUTER = "openrouter"
    DEEPSEEK = "deepseek"
    LOCAL = "local"  # For future local models support

@dataclass
class ProviderConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø²ÙˆØ¯ LLM"""
    name: Provider
    api_key_env: str
    base_url: Optional[str] = None
    headers: Optional[Dict[str, str]] = None
    models: List[str] = None
    priority: int = 0  # Lower is higher priority

# Provider configurations
PROVIDERS = {
    Provider.OPENAI: ProviderConfig(
        name=Provider.OPENAI,
        api_key_env="OPENAI_API_KEY",
        models=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
        priority=1
    ),
    Provider.GROQ: ProviderConfig(
        name=Provider.GROQ,
        api_key_env="GROQ_API_KEY",
        base_url="https://api.groq.com/openai/v1",
        models=["mixtral-8x7b-32768", "llama-2-70b-chat"],
        priority=2
    ),
    Provider.ANTHROPIC: ProviderConfig(
        name=Provider.ANTHROPIC,
        api_key_env="ANTHROPIC_API_KEY",
        models=["claude-3-sonnet", "claude-3-haiku"],
        priority=3
    ),
    Provider.GOOGLE: ProviderConfig(
        name=Provider.GOOGLE,
        api_key_env="GOOGLE_API_KEY",
        models=["gemini-pro", "gemini-pro-vision"],
        priority=4
    ),
    Provider.OPENROUTER: ProviderConfig(
        name=Provider.OPENROUTER,
        api_key_env="OPENROUTER_API_KEY",
        base_url="https://openrouter.ai/api/v1",
        headers={
            "HTTP-Referer": os.getenv("OPENROUTER_REFERRER", "https://sportsync-ai.com"),
            "X-Title": os.getenv("OPENROUTER_APP_TITLE", "SportSync_AI")
        },
        models=["openrouter/auto"],
        priority=5
    ),
    Provider.DEEPSEEK: ProviderConfig(
        name=Provider.DEEPSEEK,
        api_key_env="DEEPSEEK_API_KEY",
        base_url="https://api.deepseek.com/v1",
        models=["deepseek-chat", "deepseek-coder"],
        priority=6
    )
}

# =========================
# Environment Bootstrap
# =========================
def _bootstrap_env() -> None:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ù† .env Ùˆ streamlit secrets"""
    # Load from .env file
    try:
        from dotenv import load_dotenv
        env_paths = [
            Path.cwd() / ".env",
            Path(__file__).resolve().parent.parent / ".env",
            Path.home() / ".sportsync" / ".env"
        ]
        for env_path in env_paths:
            if env_path.exists():
                load_dotenv(env_path, override=False)
                logger.info(f"âœ… Loaded environment from: {env_path}")
                break
    except Exception as e:
        logger.warning(f"âš ï¸ Could not load .env: {e}")

    # Load from Streamlit secrets if available
    try:
        import streamlit as st
        if hasattr(st, "secrets"):
            secrets = dict(st.secrets)
            for key in secrets:
                if key.endswith("_API_KEY") and not os.getenv(key):
                    os.environ[key] = str(secrets[key])
                    logger.info(f"âœ… Loaded {key} from Streamlit secrets")
    except Exception:
        pass

_bootstrap_env()

# =========================
# Client Management
# =========================

class UnifiedLLMClient:
    """
    Ø¹Ù…ÙŠÙ„ Ù…ÙˆØ­Ø¯ Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ù…Ø²ÙˆØ¯ÙŠ LLM
    ÙŠØ¯Ø¹Ù… 141+ Ø·Ø¨Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
    """
    
    def __init__(self):
        self._clients = {}
        self._lock = threading.Lock()
        self._cache = {}
        self._initialize_clients()
        
    def _initialize_clients(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…ØªØ§Ø­ÙŠÙ†"""
        for provider, config in PROVIDERS.items():
            api_key = os.getenv(config.api_key_env)
            if api_key:
                try:
                    client = self._create_client(provider, config, api_key)
                    if client:
                        self._clients[provider] = client
                        logger.info(f"âœ… {provider.value} client initialized")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to initialize {provider.value}: {e}")
        
        if not self._clients:
            logger.error("âŒ No LLM providers available! Please set at least one API key.")
            logger.info("ğŸ’¡ Get a free API key from:")
            logger.info("   - Groq: https://console.groq.com/keys (Free tier)")
            logger.info("   - OpenAI: https://platform.openai.com/api-keys")
            logger.info("   - Google: https://makersuite.google.com/app/apikey")

    def _create_client(self, provider: Provider, config: ProviderConfig, api_key: str):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²ÙˆØ¯"""
        if provider == Provider.OPENAI:
            from openai import OpenAI
            return OpenAI(api_key=api_key)
        elif provider == Provider.GROQ:
            from openai import OpenAI  # Groq uses OpenAI-compatible API
            return OpenAI(api_key=api_key, base_url=config.base_url)
        elif provider == Provider.DEEPSEEK:
            from openai import OpenAI
            return OpenAI(api_key=api_key, base_url=config.base_url)
        elif provider == Provider.OPENROUTER:
            from openai import OpenAI
            client = OpenAI(api_key=api_key, base_url=config.base_url)
            # Store headers for later use
            client._headers = config.headers
            return client
        elif provider == Provider.ANTHROPIC:
            # Anthropic needs special handling
            return {"api_key": api_key, "provider": "anthropic"}
        elif provider == Provider.GOOGLE:
            # Google Gemini needs special handling  
            return {"api_key": api_key, "provider": "google"}
        return None

    def chat(self, 
             messages: List[Dict[str, str]], 
             model: Optional[str] = None,
             temperature: float = 0.7,
             max_tokens: int = 500,
             stream: bool = False,
             **kwargs) -> Any:
        """
        Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ fallback ØªÙ„Ù‚Ø§Ø¦ÙŠ
        Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ 141+ Ø·Ø¨Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ù†ÙØ³ÙŠ Ù„Ù„Ø±ÙŠØ§Ø¶Ø©
        """
        
        # Try providers in priority order
        errors = []
        for provider in sorted(self._clients.keys(), 
                              key=lambda p: PROVIDERS[p].priority):
            try:
                client = self._clients[provider]
                config = PROVIDERS[provider]
                
                # Select model for this provider
                if not model or model not in config.models:
                    model_to_use = config.models[0] if config.models else model
                else:
                    model_to_use = model
                
                logger.info(f"ğŸ¤– Trying {provider.value} with model {model_to_use}")
                
                # Handle different provider types
                if provider in [Provider.OPENAI, Provider.GROQ, 
                               Provider.DEEPSEEK, Provider.OPENROUTER]:
                    response = self._call_openai_compatible(
                        client, messages, model_to_use, 
                        temperature, max_tokens, stream, **kwargs
                    )
                    logger.info(f"âœ… Success with {provider.value}")
                    return response
                    
                elif provider == Provider.ANTHROPIC:
                    response = self._call_anthropic(
                        client, messages, model_to_use,
                        temperature, max_tokens, **kwargs
                    )
                    logger.info(f"âœ… Success with Anthropic")
                    return response
                    
                elif provider == Provider.GOOGLE:
                    response = self._call_google(
                        client, messages, temperature, max_tokens, **kwargs
                    )
                    logger.info(f"âœ… Success with Google")
                    return response
                    
            except Exception as e:
                error_msg = f"Provider {provider.value} failed: {str(e)}"
                errors.append(error_msg)
                logger.warning(f"âš ï¸ {error_msg}")
                continue
        
        # All providers failed
        error_report = "\n".join(errors)
        logger.error(f"âŒ All LLM providers failed:\n{error_report}")
        
        # Return a helpful fallback response
        return self._get_fallback_response(messages, errors)

    def _call_openai_compatible(self, client, messages, model, 
                                temperature, max_tokens, stream, **kwargs):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ OpenAI"""
        # Add headers for OpenRouter
        if hasattr(client, '_headers'):
            kwargs['headers'] = client._headers
            
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=stream,
            **kwargs
        )
        
        if stream:
            return response
        else:
            return response.choices[0].message.content
    
    def _call_anthropic(self, client_info, messages, model, 
                       temperature, max_tokens, **kwargs):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Anthropic Claude API"""
        # This would need anthropic SDK
        # For now, return a placeholder
        return "Anthropic integration pending. Please use OpenAI or Groq."
    
    def _call_google(self, client_info, messages, temperature, max_tokens, **kwargs):
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Google Gemini API"""
        # This would need google.generativeai SDK
        # For now, return a placeholder
        return "Google Gemini integration pending. Please use OpenAI or Groq."

    def _get_fallback_response(self, messages, errors):
        """
        Ø¥Ø±Ø¬Ø§Ø¹ Ø±Ø¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø°ÙƒÙŠ Ø¹Ù†Ø¯ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†
        ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­ØªÙ‰ Ø¨Ø¯ÙˆÙ† LLM
        """
        last_message = messages[-1]['content'] if messages else ""
        
        # Check if this is about sports recommendation
        if any(word in last_message.lower() for word in 
               ['sport', 'Ø±ÙŠØ§Ø¶Ø©', 'exercise', 'ØªÙ…Ø±ÙŠÙ†', 'activity', 'Ù†Ø´Ø§Ø·']):
            return """
            ğŸ¯ Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ SportSync AI - Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ù„Ø§ÙƒØªØ´Ø§Ù Ø±ÙŠØ§Ø¶ØªÙƒ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©.
            
            Ù„Ù„Ø£Ø³ÙØŒ Ø£ÙˆØ§Ø¬Ù‡ Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø¤Ù‚ØªØ© ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.
            
            ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø«Ù†Ø§Ø¡ØŒ ÙŠÙ…ÙƒÙ†Ùƒ:
            â€¢ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØªÙƒ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
            â€¢ Ø§Ø³ØªÙƒØ´Ø§Ù Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ù…Ù† 8000+ Ø±ÙŠØ§Ø¶Ø©
            â€¢ Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©
            
            ğŸ’¡ Ù†ØµÙŠØ­Ø©: Ø¬Ø±Ø¨ Ø§Ù„Ø±ÙŠØ§Ø¶Ø§Øª Ø§Ù„ØªÙŠ ØªØªÙ…Ø§Ø´Ù‰ Ù…Ø¹ Ø´Ø®ØµÙŠØªÙƒ:
            - Ø§Ù†Ø·ÙˆØ§Ø¦ÙŠØŸ Ø¬Ø±Ø¨ Ø§Ù„ÙŠÙˆØºØ§ØŒ Ø§Ù„Ø³Ø¨Ø§Ø­Ø©ØŒ Ø§Ù„Ø¬Ø±ÙŠ
            - Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŸ Ø¬Ø±Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù…ØŒ Ø§Ù„ÙƒØ±Ø© Ø§Ù„Ø·Ø§Ø¦Ø±Ø©ØŒ Ø§Ù„Ø±Ù‚Øµ
            - Ù…ØºØ§Ù…Ø±ØŸ Ø¬Ø±Ø¨ Ø§Ù„ØªØ³Ù„Ù‚ØŒ Ø±ÙƒÙˆØ¨ Ø§Ù„Ø£Ù…ÙˆØ§Ø¬ØŒ Ø§Ù„Ù‚ÙØ² Ø§Ù„Ù…Ø¸Ù„ÙŠ
            - Ù‡Ø§Ø¯Ø¦ØŸ Ø¬Ø±Ø¨ Ø§Ù„ØºÙˆÙ„ÙØŒ Ø§Ù„ØµÙŠØ¯ØŒ Ø§Ù„Ù…Ø´ÙŠ
            
            Ø³Ø£Ø¹ÙˆØ¯ Ù„Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù‚Ø±ÙŠØ¨Ø§Ù‹! ğŸš€
            """
        
        return f"""
        Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø£ÙˆØ§Ø¬Ù‡ Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø¤Ù‚ØªØ©. 
        
        Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙ‚Ù†ÙŠØ©:
        {chr(10).join(errors[:3])}
        
        ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù….
        """

    def get_available_providers(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­ÙŠÙ†"""
        return [p.value for p in self._clients.keys()]
    
    def health_check(self) -> Dict[str, bool]:
        """ÙØ­Øµ ØµØ­Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†"""
        status = {}
        test_message = [{"role": "user", "content": "Say 'OK' if working"}]
        
        for provider in self._clients.keys():
            try:
                response = self.chat(test_message, max_tokens=10)
                status[provider.value] = bool(response)
            except:
                status[provider.value] = False
                
        return status

# =========================
# Singleton Instance
# =========================
_client_instance = None
_client_lock = threading.Lock()

def get_llm_client() -> UnifiedLLMClient:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ù…ÙŠÙ„ LLM Ø§Ù„Ù…ÙˆØ­Ø¯ (Singleton)"""
    global _client_instance
    if _client_instance is None:
        with _client_lock:
            if _client_instance is None:
                _client_instance = UnifiedLLMClient()
    return _client_instance

# =========================
# Compatibility Functions
# =========================

def make_llm_client():
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
    return get_llm_client()

def make_llm_client_singleton():
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
    return get_llm_client()

def pick_models() -> Tuple[str, str]:
    """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
    main_models = os.getenv("CHAT_MODEL", "gpt-4o-mini,gpt-4o").split(",")
    fallback_models = os.getenv("CHAT_MODEL_FALLBACK", "gpt-3.5-turbo").split(",")
    return main_models[0].strip(), fallback_models[0].strip()

def get_models_cached():
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
    return pick_models()

def chat_once(client, messages, model=None, **kwargs):
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
    if isinstance(client, UnifiedLLMClient):
        return client.chat(messages, model=model, **kwargs)
    # Legacy support
    return client.chat.completions.create(
        model=model or "gpt-4o-mini",
        messages=messages,
        **kwargs
    ).choices[0].message.content

def get_client_and_models():
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
    client = get_llm_client()
    main_model, fallback_model = pick_models()
    return client, main_model, fallback_model

def get_streamlit_client():
    """Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Streamlit"""
    return get_llm_client()

# =========================
# Test & Debug
# =========================

def test_llm_system():
    """
    Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù…
    ÙŠØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§ÙÙ‚ 141+ Ø·Ø¨Ù‚Ø© ØªØ­Ù„ÙŠÙ„
    """
    logger.info("=" * 50)
    logger.info("ğŸ§ª SportSync AI - LLM System Test")
    logger.info("=" * 50)
    
    client = get_llm_client()
    
    # Check available providers
    providers = client.get_available_providers()
    logger.info(f"ğŸ“¦ Available providers: {providers}")
    
    if not providers:
        logger.error("âŒ No providers available!")
        logger.info("Please set at least one API key in .env file")
        return False
    
    # Test each provider
    logger.info("\nğŸ” Testing providers...")
    health = client.health_check()
    for provider, status in health.items():
        emoji = "âœ…" if status else "âŒ"
        logger.info(f"  {emoji} {provider}: {'Working' if status else 'Failed'}")
    
    # Test sports recommendation
    logger.info("\nğŸƒ Testing sports recommendation...")
    test_messages = [
        {"role": "system", "content": "Ø£Ù†Øª SportSync AI - Ø®Ø¨ÙŠØ± Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©"},
        {"role": "user", "content": "Ø£Ù†Ø§ Ø´Ø®Øµ Ø§Ù†Ø·ÙˆØ§Ø¦ÙŠ Ø£Ø­Ø¨ Ø§Ù„Ù‡Ø¯ÙˆØ¡. Ù…Ø§ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ÙŠØŸ"}
    ]
    
    try:
        response = client.chat(test_messages, temperature=0.7, max_tokens=200)
        logger.info(f"ğŸ“ Response preview: {response[:100]}...")
        logger.info("âœ… Sports recommendation working!")
        return True
    except Exception as e:
        logger.error(f"âŒ Sports recommendation failed: {e}")
        return False

if __name__ == "__main__":
    # Run test when executed directly
    test_llm_system()
