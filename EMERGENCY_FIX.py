# -*- coding: utf-8 -*-
"""
EMERGENCY FIX - SportSync AI
============================
Ø¥ØµÙ„Ø§Ø­ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø­Ø±Ø¬Ø©:
1. ØªÙØ¹ÙŠÙ„ LLM (Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† KB Ranker)
2. Streaming Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„Ù€ Chat
3. ØªÙˆØµÙŠØ§Øª Ø´Ø®ØµÙŠØ© Ù…Ø®ØªØµØ±Ø©
"""

import os
import sys

# ========================================
# Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
# ========================================

PROBLEMS = {
    "1": {
        "issue": "âŒ LLM Ù…Ø¹Ø·Ù„ - Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… KB Ranker (Ø¨Ø·Ø§Ù‚Ø§Øª Ø¬Ø§Ù‡Ø²Ø©)",
        "location": "core/backend_gpt.py:1804",
        "cause": "Models ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: o4-mini, gpt-5",
        "fix": "Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¨Ù€: gpt-4o-mini, gpt-4o"
    },
    "2": {
        "issue": "âŒ Chat ÙŠØ¹Ø±Ø¶ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (Ø¨Ø¯ÙˆÙ† streaming)",
        "location": "quiz_service/app.py:492",
        "cause": "Buffering ÙƒÙ„ Ø§Ù„Ù€ chunks Ø«Ù… Ø¹Ø±Ø¶Ù‡Ø§ Ù…Ø¹Ø§Ù‹",
        "fix": "Ø¹Ø±Ø¶ ÙƒÙ„ chunk ÙÙˆØ± ÙˆØµÙˆÙ„Ù‡"
    },
    "3": {
        "issue": "âŒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ (800+ ÙƒÙ„Ù…Ø©)",
        "location": "core/complete_sport_system.py:148",
        "cause": "System prompts Ø·ÙˆÙŠÙ„Ø© ÙˆØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©",
        "fix": "ØªÙ‚ØµÙŠØ± max_tokens + system prompts Ù…Ø®ØªØµØ±Ø©"
    }
}

# ========================================
# Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª
# ========================================

def fix_1_enable_llm():
    """Ø¥ØµÙ„Ø§Ø­ 1: ØªÙØ¹ÙŠÙ„ LLM"""
    env_path = "/Users/mohammadal-saati/SportSync_AI-1/.env"
    
    print("\nğŸ”§ Fixing Problem 1: Enabling LLM...")
    
    # Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
    with open(env_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
    content = content.replace(
        "CHAT_MODEL_DISCOVERY=o4-mini",
        "CHAT_MODEL_DISCOVERY=gpt-4o-mini"
    )
    content = content.replace(
        "CHAT_MODEL_REASONING=gpt-5",
        "CHAT_MODEL_REASONING=gpt-4o"
    )
    content = content.replace(
        "REASONING_MODEL=gpt-5",
        "REASONING_MODEL=gpt-4o"
    )
    content = content.replace(
        "INTELLIGENCE_MODEL=o4-mini",
        "INTELLIGENCE_MODEL=gpt-4o-mini"
    )
    
    # Ø§Ù„ÙƒØªØ§Ø¨Ø©
    with open(env_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("âœ… .env updated with correct models")
    print("   - Discovery: gpt-4o-mini")
    print("   - Reasoning: gpt-4o")


def fix_2_real_streaming():
    """Ø¥ØµÙ„Ø§Ø­ 2: Streaming Ø­Ù‚ÙŠÙ‚ÙŠ"""
    app_path = "/Users/mohammadal-saati/SportSync_AI-1/quiz_service/app.py"
    
    print("\nğŸ”§ Fixing Problem 2: Real Streaming...")
    
    with open(app_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
    old_code = """                for chunk in start_dynamic_chat_stream(
                        answers=answers,
                        previous_recommendation=recs_for_chat,
                        ratings=ratings,
                        user_id="web_user",
                        lang=lang,
                        chat_history=st.session_state["chat_history"],
                        user_message=user_text
                    ):
                        buf.append(_safe_str(chunk))
                        if LIVE_TYPING:
                            ph.markdown("".join(_safe_str(x) for x in buf))
                    reply = "".join(_safe_str(x) for x in buf).strip()"""
    
    # Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ - streaming Ø­Ù‚ÙŠÙ‚ÙŠ
    new_code = """                for chunk in start_dynamic_chat_stream(
                        answers=answers,
                        previous_recommendation=recs_for_chat,
                        ratings=ratings,
                        user_id="web_user",
                        lang=lang,
                        chat_history=st.session_state["chat_history"],
                        user_message=user_text
                    ):
                        buf.append(_safe_str(chunk))
                        # Ø¹Ø±Ø¶ ÙÙˆØ±ÙŠ Ù„ÙƒÙ„ chunk - Ø¨Ø¯ÙˆÙ† Ø§Ù†ØªØ¸Ø§Ø±
                        ph.markdown("".join(_safe_str(x) for x in buf))
                    reply = "".join(_safe_str(x) for x in buf).strip()"""
    
    content = content.replace(old_code, new_code)
    
    with open(app_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("âœ… app.py updated with real streaming")
    print("   - Removed conditional: if LIVE_TYPING")
    print("   - Now shows each chunk immediately")


def fix_3_compact_recommendations():
    """Ø¥ØµÙ„Ø§Ø­ 3: ØªÙˆØµÙŠØ§Øª Ù…Ø®ØªØµØ±Ø©"""
    system_path = "/Users/mohammadal-saati/SportSync_AI-1/core/complete_sport_system.py"
    
    print("\nğŸ”§ Fixing Problem 3: Compact Recommendations...")
    
    with open(system_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ max_tokens
    content = content.replace(
        "max_tokens=2000",
        "max_tokens=600  # COMPACT: reduced from 2000"
    )
    
    with open(system_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("âœ… complete_sport_system.py updated")
    print("   - max_tokens: 2000 â†’ 600")
    print("   - System prompts already updated (previous commit)")


def run_all_fixes():
    """ØªØ·Ø¨ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª"""
    print("="*60)
    print("ğŸš¨ EMERGENCY FIX - SportSync AI")
    print("="*60)
    
    print("\nğŸ“‹ Problems detected:")
    for key, problem in PROBLEMS.items():
        print(f"\n{key}. {problem['issue']}")
        print(f"   Location: {problem['location']}")
        print(f"   Cause: {problem['cause']}")
        print(f"   Fix: {problem['fix']}")
    
    print("\n" + "="*60)
    input("Press ENTER to apply ALL fixes... ")
    
    try:
        fix_1_enable_llm()
        fix_2_real_streaming()
        fix_3_compact_recommendations()
        
        print("\n" + "="*60)
        print("âœ… ALL FIXES APPLIED SUCCESSFULLY!")
        print("="*60)
        
        print("\nğŸš€ Next steps:")
        print("1. git add .")
        print("2. git commit -m 'fix: Enable LLM, real streaming, compact recommendations'")
        print("3. git push origin main")
        print("4. Render will auto-deploy")
        print("5. Monitor logs: https://dashboard.render.com/")
        
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        sys.exit(1)


if __name__ == "__main__":
    run_all_fixes()
