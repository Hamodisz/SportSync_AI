# -*- coding: utf-8 -*-
"""
tests/test_scoring_system.py
-----------------------------
Comprehensive tests for Task 2.1: Question Scoring System Improvement

Tests the new explicit scoring system where each question option
has explicit Z-axis scores instead of keyword-based inference.
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from layer_z_engine import calculate_z_scores_from_questions
import json


def test_basic_scoring():
    """Test 1: Basic scoring calculation"""
    print("\nðŸ§ª Test 1: Basic Scoring Calculation")

    # Sample answers
    answers = {
        "q1": {"answer": ["ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"]},  # Calm focus
        "_session_id": "test-1"
    }

    # Use sample file
    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Verify structure
    assert isinstance(scores, dict), "Should return a dict"
    assert len(scores) > 0, "Should have at least one score"
    print(f"âœ… Calculated {len(scores)} Z-axis scores")

    # Verify scores are in valid range
    for axis, score in scores.items():
        if axis == "sensory_sensitivity":  # Unipolar
            assert 0.0 <= score <= 1.0, f"{axis} should be 0.0-1.0, got {score}"
        else:  # Bipolar
            assert -1.0 <= score <= 1.0, f"{axis} should be -1.0-1.0, got {score}"

    print("âœ… All scores in valid range")
    print("âœ… Test 1 PASSED\n")


def test_expected_scores():
    """Test 2: Verify expected scores for known answer"""
    print("\nðŸ§ª Test 2: Expected Scores for Known Answer")

    # Q1, Option 1: "ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"
    # Expected scores from JSON:
    # calm_adrenaline: -0.8 (very calm)
    # solo_group: -0.6 (solo)
    # sensory_sensitivity: 0.7 (high sensitivity)
    # control_freedom: -0.5 (controlled)

    answers = {
        "q1": {"answer": ["ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"]},
        "_session_id": "test-2"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Q1 has weight=3, but only one answer, so scores should match JSON exactly
    expected = {
        "calm_adrenaline": -0.8,
        "solo_group": -0.6,
        "sensory_sensitivity": 0.7,
        "control_freedom": -0.5
    }

    for axis, expected_score in expected.items():
        assert axis in scores, f"Missing axis: {axis}"
        actual = scores[axis]
        # Allow small floating point difference
        assert abs(actual - expected_score) < 0.01, \
            f"{axis}: expected {expected_score}, got {actual}"
        print(f"âœ… {axis}: {actual:+.2f} (expected {expected_score:+.2f})")

    print("âœ… Test 2 PASSED\n")


def test_weighted_average():
    """Test 3: Weighted average calculation"""
    print("\nðŸ§ª Test 3: Weighted Average Calculation")

    # Two answers with different weights
    answers = {
        "q1": {"answer": ["ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"]},  # weight=3, calm=-0.8
        "q2": {"answer": ["Ø§Ù„Ù…Ù„Ù„"]},  # weight=2, different scores
        "_session_id": "test-3"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Should have multiple axes
    assert len(scores) > 0, "Should calculate scores"

    # Weighted average should be calculated correctly
    # Q1 (weight=3): calm_adrenaline=-0.8
    # Q2 (weight=2): has no calm_adrenaline score (only repeat_variety, sensory_sensitivity, compete_enjoy)
    # So calm_adrenaline should still be -0.8

    if "calm_adrenaline" in scores:
        assert abs(scores["calm_adrenaline"] - (-0.8)) < 0.01, \
            f"calm_adrenaline should be -0.8, got {scores['calm_adrenaline']}"
        print(f"âœ… calm_adrenaline: {scores['calm_adrenaline']:+.2f}")

    # repeat_variety should come only from Q2 (weight=2)
    # Q2, Option 1 "Ø§Ù„Ù…Ù„Ù„": repeat_variety=0.7
    if "repeat_variety" in scores:
        assert abs(scores["repeat_variety"] - 0.7) < 0.01, \
            f"repeat_variety should be 0.7, got {scores['repeat_variety']}"
        print(f"âœ… repeat_variety: {scores['repeat_variety']:+.2f}")

    print("âœ… Test 3 PASSED\n")


def test_multiple_answers_same_axis():
    """Test 4: Multiple answers affecting same axis"""
    print("\nðŸ§ª Test 4: Multiple Answers on Same Axis")

    # Q1, Option 1: calm_adrenaline=-0.8 (weight=3)
    # Q6, Option 4: calm_adrenaline=+0.8 (weight=2)
    # Weighted average: (-0.8*3 + 0.8*2) / (3+2) = (-2.4+1.6)/5 = -0.8/5 = -0.16

    answers = {
        "q1": {"answer": ["ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"]},  # calm=-0.8, w=3
        "q6": {"answer": ["Ø£Ù…Ø§ÙƒÙ† ÙÙŠÙ‡Ø§ ØªØ­Ø¯ÙŠ"]},  # calm=+0.8, w=2
        "_session_id": "test-4"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Check calm_adrenaline weighted average
    if "calm_adrenaline" in scores:
        expected = (-0.8 * 3 + 0.8 * 2) / 5
        actual = scores["calm_adrenaline"]
        assert abs(actual - expected) < 0.01, \
            f"calm_adrenaline: expected {expected:.2f}, got {actual:.2f}"
        print(f"âœ… calm_adrenaline: {actual:+.2f} (expected {expected:+.2f})")

    print("âœ… Test 4 PASSED\n")


def test_all_sample_questions():
    """Test 5: Answer all 6 sample questions"""
    print("\nðŸ§ª Test 5: All Sample Questions")

    answers = {
        "q1": {"answer": ["ØªÙØ§Ø¹Ù„ ÙˆØ³Ø±Ø¹Ø©"]},  # Speed and interaction
        "q2": {"answer": ["ØºÙŠØ§Ø¨ Ù†ØªÙŠØ¬Ø©"]},  # Lack of results
        "q3": {"answer": ["Ø£Ø¬Ø±Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø©"]},  # Try immediately
        "q4": {"answer": ["Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø©"]},  # With group
        "q5": {"answer": ["Ù…Ù†Ø§ÙØ³Ø© Ù…Ø¨Ø§Ø´Ø±Ø©"]},  # Direct competition
        "q6": {"answer": ["Ø£Ù…Ø§ÙƒÙ† ÙÙŠÙ‡Ø§ ØªØ­Ø¯ÙŠ"]},  # Challenging places
        "_session_id": "test-5"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Should have calculated scores for all relevant axes
    assert len(scores) > 0, "Should have scores"
    print(f"âœ… Calculated {len(scores)} Z-axis scores from 6 questions")

    # All scores should be in valid range
    for axis, score in sorted(scores.items()):
        if axis == "sensory_sensitivity":
            assert 0.0 <= score <= 1.0, f"{axis} out of range: {score}"
        else:
            assert -1.0 <= score <= 1.0, f"{axis} out of range: {score}"
        print(f"   {axis}: {score:+.2f}")

    # Based on the answers, we expect:
    # - High adrenaline (speed, direct competition, challenging places)
    # - Group preference (Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø©)
    # - High competition (direct competition)
    # - Intuitive approach (try immediately)

    if "calm_adrenaline" in scores:
        assert scores["calm_adrenaline"] > 0, "Should favor adrenaline"
        print(f"âœ… Adrenaline preference detected: {scores['calm_adrenaline']:+.2f}")

    if "solo_group" in scores:
        assert scores["solo_group"] > 0, "Should favor group"
        print(f"âœ… Group preference detected: {scores['solo_group']:+.2f}")

    if "compete_enjoy" in scores:
        assert scores["compete_enjoy"] > 0, "Should favor competition"
        print(f"âœ… Competition preference detected: {scores['compete_enjoy']:+.2f}")

    print("âœ… Test 5 PASSED\n")


def test_empty_answers():
    """Test 6: Handle empty answers gracefully"""
    print("\nðŸ§ª Test 6: Empty Answers Handling")

    answers = {"_session_id": "test-6"}

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Should return empty dict, not crash
    assert isinstance(scores, dict), "Should return dict"
    assert len(scores) == 0, "Should be empty for no answers"
    print("âœ… Empty answers handled gracefully")
    print("âœ… Test 6 PASSED\n")


def test_partial_match():
    """Test 7: Partial text matching"""
    print("\nðŸ§ª Test 7: Partial Text Matching")

    # User might type partial answer or it might be truncated
    answers = {
        "q1": {"answer": ["ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦"]},  # Partial match of "ØªØ±ÙƒÙŠØ² Ù‡Ø§Ø¯Ø¦ Ø¹Ù„Ù‰ ØªÙØµÙŠÙ„Ø© ÙˆØ§Ø­Ø¯Ø©"
        "_session_id": "test-7"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Should still match due to normalization
    assert len(scores) > 0, "Should match partial text"
    assert "calm_adrenaline" in scores, "Should find calm_adrenaline score"
    print(f"âœ… Partial match successful: {len(scores)} scores calculated")
    print("âœ… Test 7 PASSED\n")


def test_english_answers():
    """Test 8: English language support"""
    print("\nðŸ§ª Test 8: English Language Support")

    answers = {
        "q1": {"answer": ["Calm focus on a single detail"]},
        "_session_id": "test-8"
    }

    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions_v2_sample.json",  # Same file has both ar/en
        lang="English"
    )

    # Should match English text
    assert len(scores) > 0, "Should match English text"
    assert "calm_adrenaline" in scores, "Should find scores from English option"

    expected_calm = -0.8  # Same as Arabic version
    assert abs(scores["calm_adrenaline"] - expected_calm) < 0.01, \
        f"Expected {expected_calm}, got {scores['calm_adrenaline']}"

    print(f"âœ… English matching successful: {len(scores)} scores")
    print("âœ… Test 8 PASSED\n")


def test_json_structure():
    """Test 9: Verify JSON structure is correct"""
    print("\nðŸ§ª Test 9: JSON Structure Validation")

    # Load and validate JSON structure
    with open("arabic_questions_v2_sample.json", "r", encoding="utf-8") as f:
        questions = json.load(f)

    assert isinstance(questions, list), "Questions should be a list"
    assert len(questions) == 6, f"Should have 6 questions, got {len(questions)}"
    print(f"âœ… Found {len(questions)} questions")

    # Validate each question
    for i, q in enumerate(questions, 1):
        # Required fields
        assert "key" in q, f"Q{i} missing 'key'"
        assert "question_ar" in q, f"Q{i} missing 'question_ar'"
        assert "question_en" in q, f"Q{i} missing 'question_en'"
        assert "options" in q, f"Q{i} missing 'options'"
        assert "bucket" in q, f"Q{i} missing 'bucket'"
        assert "weight" in q, f"Q{i} missing 'weight'"

        # Validate options
        options = q["options"]
        assert isinstance(options, list), f"Q{i} options should be list"
        assert len(options) >= 2, f"Q{i} should have at least 2 options"

        for j, opt in enumerate(options):
            assert "text_ar" in opt, f"Q{i} Option{j+1} missing text_ar"
            assert "text_en" in opt, f"Q{i} Option{j+1} missing text_en"
            assert "scores" in opt, f"Q{i} Option{j+1} missing scores"

            scores = opt["scores"]
            assert isinstance(scores, dict), f"Q{i} Option{j+1} scores should be dict"
            assert len(scores) > 0, f"Q{i} Option{j+1} should have at least one score"

            # Validate score ranges
            for axis, score in scores.items():
                if axis == "sensory_sensitivity":
                    assert 0.0 <= score <= 1.0, \
                        f"Q{i} Option{j+1} {axis}={score} out of range [0,1]"
                else:
                    assert -1.0 <= score <= 1.0, \
                        f"Q{i} Option{j+1} {axis}={score} out of range [-1,1]"

        print(f"âœ… Q{i} structure valid ({len(options)} options)")

    print("âœ… Test 9 PASSED\n")


def test_backward_compatibility():
    """Test 10: Old format detection and fallback"""
    print("\nðŸ§ª Test 10: Backward Compatibility")

    # Test with old format (if it exists)
    answers = {
        "q1": {"answer": ["test"]},
        "_session_id": "test-10"
    }

    # Try with old format file (should detect and return empty)
    scores = calculate_z_scores_from_questions(
        answers,
        questions_file="arabic_questions.json",  # Old format
        lang="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
    )

    # Should detect old format and return empty dict
    assert isinstance(scores, dict), "Should return dict"
    print("âœ… Old format detection works")
    print("âœ… Test 10 PASSED\n")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ðŸš€ Task 2.1 Scoring System Tests")
    print("="*70)

    test_basic_scoring()
    test_expected_scores()
    test_weighted_average()
    test_multiple_answers_same_axis()
    test_all_sample_questions()
    test_empty_answers()
    test_partial_match()
    test_english_answers()
    test_json_structure()
    test_backward_compatibility()

    print("="*70)
    print("âœ… ALL TESTS PASSED!")
    print("="*70 + "\n")
