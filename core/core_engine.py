# -- coding: utf-8 --
from __future__ import annotations

import logging
from pathlib import Path
from typing import Dict, List, Optional
import re

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s | %(asctime)s | core_engine | %(message)s"
)

# Ù…Ø³Ø§Ø±Ø§Øª Ù‚ÙŠØ§Ø³ÙŠØ©
IMAGES_DIR = Path("content_studio/ai_images/outputs/")
VOICE_PATH = Path("content_studio/ai_voice/voices/final_voice.mp3")
FINAL_VIDS_DIR = Path("content_studio/ai_video/final_videos/")
BURNED_DIR = Path("content_studio/ai_video/_burned_images/")

for p in (IMAGES_DIR, VOICE_PATH.parent, FINAL_VIDS_DIR, BURNED_DIR):
    p.mkdir(parents=True, exist_ok=True)

# --------- Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ù…Ø±Ù†Ø© ----------
_generate_script_fn = None
try:
    from content_studio.generate_script.script_generator import generate_script as _generate_script_fn
except Exception:
    pass

_generate_images_fn = None
try:
    from content_studio.ai_images.generate_images import generate_images as _generate_images_fn
except Exception:
    pass

_generate_voice_fn = None
try:
    from content_studio.ai_voice.voice_generator import generate_voice_from_script as _generate_voice_fn
except Exception:
    pass

_compose_video_fn = None
try:
    # Ø¥Ù† ÙƒØ§Ù† Ø¹Ù†Ø¯Ùƒ ÙƒÙˆÙ…Ø¨ÙˆØ²Ø± Ù…Ø®ØµØµ
    from content_studio.ai_video.video_composer import compose_video_from_assets as _compose_video_fn
except Exception:
    pass

# ======== ØªØ±ÙƒÙŠØ¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Fallback Ù…Ø­Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MoviePy) ========
def _natural_key(p: Path):
    """ØªØ±ØªÙŠØ¨ Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù…Ù„ÙØ§Øª Ù…Ø«Ù„ scene_1, scene_2..."""
    stem = p.stem
    try:
        n = int(stem.split("_")[-1])
    except Exception:
        n = 10**9
    return (stem.split("_")[0], n, p.name)

# --------- Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù†ØµÙˆØµ (captions) ----------
def _extract_scenes(script_text: str) -> List[str]:
    """ÙŠÙØµÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø¥Ù„Ù‰ Ù…Ø´Ø§Ù‡Ø¯ (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)."""
    parts = re.split(r"(?:Scene\s*#?\d*[:\-]?\s*)|(?:Ù…Ø´Ù‡Ø¯\s*#?\d*[:\-]?\s*)",
                     script_text, flags=re.IGNORECASE)
    scenes = [p.strip() for p in parts if p and p.strip()]
    if not scenes:
        scenes = [p.strip() for p in re.split(r"\n\s*\n", script_text) if p.strip()]
    return scenes


def _wrap_lines(text: str, max_len: int = 36) -> List[str]:
    words = text.split()
    out, line = [], ""
    for w in words:
        nxt = (line + " " + w).strip()
        if len(nxt) <= max_len:
            line = nxt
        else:
            if line:
                out.append(line)
            line = w
    if line:
        out.append(line)
    return out[:8]


def _burn_captions_on_images(images: List[Path], captions: List[str]) -> List[Path]:
    """ÙŠØ­Ø±Ù‚ Ø¹Ø¨Ø§Ø±Ø§Øª captions Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ± ÙˆÙŠØ­ÙØ¸Ù‡Ø§ ÙÙŠ BURNED_DIR Ø¨Ù†ÙØ³ Ø§Ù„ØªØ±ØªÙŠØ¨."""
    from PIL import Image, ImageDraw, ImageFont

    # Ù†Ø¸Ù‘Ù Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø­Ø±ÙˆÙ‚Ø©
    for f in BURNED_DIR.glob("*"):
        try:
            f.unlink()
        except Exception:
            pass

    out_paths: List[Path] = []
    for i, img_path in enumerate(images):
        cap = captions[i] if i < len(captions) else ""
        im = Image.open(img_path).convert("RGBA")

        # Ø·Ø¨Ù‚Ø© Ø´ÙØ§ÙØ© Ù„Ù„Ù†Øµ
        overlay = Image.new("RGBA", im.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)

        # Ø®Ø·
        try:
            font = ImageFont.truetype("arial.ttf", max(24, im.size[0] // 32))
        except Exception:
            font = ImageFont.load_default()

        lines = _wrap_lines(cap, max_len=max(24, im.size[0] // 28))
        if lines:
            # ØµÙ†Ø¯ÙˆÙ‚ Ø´Ø¨Ù‡ Ø´ÙØ§Ù Ø£Ø³ÙÙ„ Ø§Ù„ØµÙˆØ±Ø©
            pad = 24
            line_h = (font.size + 8)
            box_h = pad * 2 + line_h * len(lines)

            # Ù…Ø³ØªØ·ÙŠÙ„ Ø®Ù„ÙÙŠ
            rect_y0 = im.size[1] - box_h
            rect_y1 = im.size[1]
            draw.rectangle([(0, rect_y0), (im.size[0], rect_y1)], fill=(0, 0, 0, 140))

            # Ø§Ø±Ø³Ù… Ø§Ù„Ø³Ø·ÙˆØ±
            y = rect_y0 + pad
            x = pad
            for ln in lines:
                # Ø¸Ù„ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ
                draw.text((x+2, y+2), ln, font=font, fill=(0, 0, 0, 200))
                draw.text((x, y), ln, font=font, fill=(255, 255, 255, 255))
                y += line_h

        burned = Path(BURNED_DIR / f"{img_path.stem}_cap.png")
        im_out = Image.alpha_composite(im, overlay).convert("RGB")
        burned.parent.mkdir(parents=True, exist_ok=True)
        im_out.save(burned, format="PNG")
        out_paths.append(burned)

    return out_paths


def _fallback_compose_video(
    image_duration: int = 4,
    fps: int = 30,
    voice: Optional[str] = None,
    captions: Optional[List[str]] = None
) -> str:
    """ØªØµÙ…ÙŠÙ… ÙÙŠØ¯ÙŠÙˆ Ø¨Ø³ÙŠØ· Ù…Ù† Ø§Ù„ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… MoviePy. ÙŠØ¯Ø¹Ù… Ø­Ø±Ù‚ captions Ù‚Ø¨Ù„ Ø§Ù„ØªØ¬Ù…ÙŠØ¹."""
    from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip
    images = sorted(list(IMAGES_DIR.glob(".png")) + list(IMAGES_DIR.glob(".jpg")))
    if not images:
        raise ValueError("Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙˆØ± ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬.")
    clips = [ImageClip(str(p)).set_duration(image_duration) for p in images]
    video = concatenate_videoclips(clips, method="compose")

    if voice and Path(voice).exists():
        video = video.set_audio(AudioFileClip(str(voice)))
    out = FINAL_VIDS_DIR / "final_video.mp4"
    video.write_videofile(str(out), fps=fps, codec="libx264", audio_codec="aac")
    return str(out)

# Ù†ÙØ±Ø¶ Ø§Ù„ÙÙˆÙ„Ø¨Ø§Ùƒ Ø­ØªÙ‰ Ù„Ùˆ ÙˆÙØ¬Ø¯ ÙƒÙˆÙ…Ø¨ÙˆØ²Ø± Ø®Ø§Ø±Ø¬ÙŠ (Ø¹Ù„Ø´Ø§Ù† Ù†Ø¶Ù…Ù† ÙŠØ´ØªØºÙ„ Ø§Ù„Ø¢Ù†)
_compose_video_fn = _fallback_compose_video


def _ensure_tools_available() -> List[str]:
    missing: List[str] = []
    if _generate_images_fn is None:
        missing.append("generate_images")
    # Ø§Ù„Ø³ÙƒØ±Ø¨Øª/Ø§Ù„ØµÙˆØª Ø§Ø®ØªÙŠØ§Ø±ÙŠØ§Ù†ØŒ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¹Ù†Ø¯Ù†Ø§ ÙÙˆÙ„Ø¨Ø§Ùƒ
    return missing


def quick_diagnose() -> Dict:
    return {
        "images_dir_exists": IMAGES_DIR.exists(),
        "images_count": len(list(IMAGES_DIR.glob("*.png"))) + len(list(IMAGES_DIR.glob("*.jpg"))),
        "voice_exists": VOICE_PATH.exists(),
        "voice_size": VOICE_PATH.stat().st_size if VOICE_PATH.exists() else 0,
        "final_videos_dir_exists": FINAL_VIDS_DIR.exists(),
        "tools_missing": _ensure_tools_available(),
    }


def run_full_generation(
    user_data: Dict,
    lang: str = "en",                 # ğŸ” Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ø¢Ù† Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
    image_duration: int = 4,
    override_script: Optional[str] = None,
    mute_if_no_voice: bool = True,
    skip_cleanup: bool = True,
) -> Dict:
    """ÙŠØ´ØºÙ‘Ù„ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬: Script -> Images -> (Voice) -> Video (Ù…Ø¹ captions)."""
    try:
        # ØªÙ†Ø¸ÙŠÙ ØµÙˆØ± Ù‚Ø¯ÙŠÙ…Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        if not skip_cleanup and IMAGES_DIR.exists():
            for f in IMAGES_DIR.glob("*"):
                try:
                    f.unlink()
                except Exception:
                    pass

        # 1) Ø³ÙƒØ±Ø¨Øª
        if override_script and override_script.strip():
            script = override_script.strip()
            logging.info("ğŸ“ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ Ø³ÙƒØ±Ø¨Øª Ø¬Ø§Ù‡Ø² (override_script).")
        else:
            if _generate_script_fn:
                # Ø¹Ø¯Ù‘Ù„ Ø§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø­Ø³Ø¨ Ø¯Ø§Ù„ØªÙƒ Ù„Ùˆ Ø§Ø®ØªÙ„Ù
                script = _generate_script_fn(
                    topic=user_data.get("topic", "sports motivation"),
                    tone=user_data.get("traits", {}).get("tone", "emotional"),
                    lang="english" if lang.lower().startswith("en") else "arabic",
                )
            else:
                script = (
                    "Scene 1: Start with a small step.\n\n"
                    "Scene 2: Keep going when no one is watching.\n\n"
                    "Scene 3: Results come to those who donâ€™t quit."
                )

        captions = _extract_scenes(script)

        # 2) ØµÙˆØ±
        if _generate_images_fn is None:
            raise RuntimeError("Ø¯Ø§Ù„Ø© generate_images ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©.")
        images = _generate_images_fn(script, lang)
        if not images:
            raise RuntimeError("Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ ØµÙˆØ±.")

        # 3) ØµÙˆØª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        voice_path = None
        if _generate_voice_fn:
            try:
                voice_path = generate_voice_fn(script, lang) if _generate_voice_fn.code_.co_argcount >= 2 else _generate_voice_fn(script)
            except Exception as e:
                logging.warning(f"ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª: {e}")
                if not mute_if_no_voice:
                    raise

        # 4) ÙÙŠØ¯ÙŠÙˆ
        if _compose_video_fn:
            try:
                # ÙˆÙ‚Ù‘Ø¹ Ø¯Ø§Ù„ØªÙƒ Ø¥Ù† ÙƒØ§Ù† Ù…Ø®ØªÙ„ÙØ§Ù‹
                video_path = _compose_video_fn(image_duration=image_duration, voice_path=voice_path)
            except TypeError:
                video_path = _compose_video_fn(image_duration=image_duration)
        else:
            video_path = _fallback_compose_video(image_duration=image_duration, voice=voice_path)

        return {
            "script": str(script),
            "images": [str(p) for p in sorted(IMAGES_DIR.glob("*.png")) + sorted(IMAGES_DIR.glob("*.jpg"))],
            "voice": str(voice_path) if voice_path else None,
            "video": str(video_path),
            "error": None,
        }

    except Exception as e:
        logging.error(f"ğŸ”¥ ÙØ´Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„: {e}")
        return {"script": None, "images": [], "voice": None, "video": None, "error": str(e)}
