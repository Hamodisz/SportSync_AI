# -- coding: utf-8 --
"""
core/backend_gpt.py
-------------------
Sport identity recommendations (3 cards) with Layer-Z, first-week (qualitative),
and VR/no-VR variants. Arabic/English.

Pipeline (ÙˆØ§Ù‚Ø¹ÙŠ Ø£ÙˆÙ„Ø§Ù‹):
1) Evidence Gate (ÙŠØ±ÙØ¶ Ø¥Ù† ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© ÙˆÙŠØ¹ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø©)
2) ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (priors + trait_links + guards + templates)
3) LLM ÙƒØ¢Ø®Ø± Ø®ÙŠØ§Ø± (fallback) Ø¥Ù† Ù…Ø§ ÙƒÙØª Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© + Ø¥ØµÙ„Ø§Ø­ JSON Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…
4) Hard de-dup + URL scrub + Telemetry + Ù‚ÙÙ„ Ù…Ù„Ù Ù„Ù„Ù€ blacklist (thread/process-safe)

Ù‚ÙˆØ§Ø¹Ø¯ Ø«Ø§Ø¨ØªØ©:
- Ù…Ù…Ù†ÙˆØ¹ Ø°ÙƒØ± Ø§Ù„ÙˆÙ‚Øª/Ø§Ù„ØªÙƒÙ„ÙØ©/Ø§Ù„Ø¹Ø¯Ù‘Ø§Øª/Ø§Ù„Ø¬ÙˆÙ„Ø§Øª/Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø¨Ø§Ø´Ø±.
- ÙŠÙØ³Ù…Ø­ Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø±ÙŠØ§Ø¶Ø§Øª Ø¥Ø°Ø§ ALLOW_SPORT_NAMES=True
- dedupe Ù…Ø­Ù„ÙŠ ÙˆØ¹Ø§Ù„Ù…ÙŠ Ø¹Ø¨Ø± data/blacklist.json
"""

from __future__ import annotations

import os, json, re, hashlib, importlib, time, random
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

# ========= Job Manager (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) =========
try:
    from core.job_manager import create_job, read_job, update as job_update, run_in_thread
except Exception:
    def create_job(meta: Optional[dict] = None): return {"id": "nojob", "status": "error"}
    def read_job(job_id: str): return None
    def job_update(job_id: str, **kw): pass
    def run_in_thread(job_id: str, target, *args, **kwargs): return None

def _job_note(job_id: str,
              progress: Optional[int] = None,
              note: Optional[str] = None,
              status: Optional[str] = None) -> None:
    if not job_id:
        return
    try:
        payload: Dict[str, Any] = {}
        if progress is not None: payload["progress"] = int(progress)
        if note: payload["note"] = note
        if status: payload["status"] = status
        if payload:
            job_update(job_id, **payload)
    except Exception as e:
        print(f"[JOB_NOTE][WARN] {e}")

# ========= LLM Client (OpenAI-compatible Ø¹Ø¨Ø± Ø·Ø¨Ù‚Ø© Ù…ÙØµÙˆÙ„Ø©) =========
try:
    from core.llm_client import make_llm_client, pick_models, chat_once
except Exception as e:
    raise RuntimeError("Ù…Ù„Ù core/llm_client.py Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ÙÙŠÙ‡ Ø®Ø·Ø£. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡.") from e

LLM_CLIENT = make_llm_client()
try:
    CHAT_MODEL, CHAT_MODEL_FALLBACK = pick_models()  # ÙŠØ±Ø¬Ù‘Ø¹ Tuple
except Exception:
    _models = pick_models()
    CHAT_MODEL = getattr(_models, "get", lambda *_: None)("main")
    CHAT_MODEL_FALLBACK = getattr(_models, "get", lambda *_: None)("fallback")

print(f"[BOOT] LLM READY? {'YES' if LLM_CLIENT else 'NO'} | main={CHAT_MODEL} fb={CHAT_MODEL_FALLBACK}")

# ========= App Config =========
try:
    from core.app_config import get_config
    CFG = get_config()
except Exception:
    CFG = {}

ALLOW_SPORT_NAMES = (CFG.get("recommendations") or {}).get("allow_sport_names", True)

REC_RULES = CFG.get("recommendations") or {}
_MIN_CHARS = int(REC_RULES.get("min_chars", 220))
_REQUIRE_WIN = bool(REC_RULES.get("require_win_condition", True))
_MIN_CORE_SKILLS = int(REC_RULES.get("min_core_skills", 3))

# ========= Runtime Guards / Tunables =========
REC_BUDGET_S = float(os.getenv("REC_BUDGET_S", "22"))
REC_REPAIR_ENABLED = os.getenv("REC_REPAIR_ENABLED", "1") == "1"
REC_FAST_MODE = os.getenv("REC_FAST_MODE", "0") == "1"
REC_DEBUG = os.getenv("REC_DEBUG", "0") == "1"
MAX_PROMPT_CHARS = int(os.getenv("MAX_PROMPT_CHARS", "6000"))

def _dbg(msg: str) -> None:
    if REC_DEBUG:
        print(f"[RECDBG] {msg}")

def _warn(msg: str) -> None:
    print(f"[WARN] {msg}")

def _err(msg: str) -> None:
    print(f"[ERROR] {msg}")

# Evidence Gate thresholds (defaults if not provided)
EGCFG = (CFG.get("analysis") or {}).get("egate", {}) if isinstance(CFG.get("analysis"), dict) else {}
_EG_MIN_ANSWERS = int(EGCFG.get("min_answered", 3))
_EG_MIN_TOTAL_CHARS = int(EGCFG.get("min_total_chars", 120))
_EG_REQUIRED_KEYS = list(EGCFG.get("required_keys", []))

# ========= DataPipe (Zapier/Webhook/Disk) =========
try:
    from core.data_pipe import get_pipe
    _PIPE = get_pipe()
except Exception:
    _PIPE = None

# ========= Security scrubber =========
try:
    from core.security import scrub_unknown_urls
except Exception:
    def scrub_unknown_urls(text_or_card: str, cfg: Dict[str, Any]) -> str:
        return text_or_card

# ========= Evidence Gate (external) + fallback =========
try:
    from core.evidence_gate import evaluate as egate_evaluate
except Exception:
    egate_evaluate = None

# ========= Paths / Data =========
try:
    HERE = Path(__file__).resolve().parent
except NameError:
    HERE = Path.cwd()
ROOT = HERE.parent if HERE.name == "core" else HERE
DATA_DIR = (ROOT / "data").resolve()
DATA_DIR.mkdir(parents=True, exist_ok=True)

BL_PATH = DATA_DIR / "blacklist.json"
KB_PATH = DATA_DIR / "sportsync_knowledge.json"
AL_PATH = DATA_DIR / "labels_aliases.json"

# ========= File Lock (race-safe) =========
try:
    from filelock import FileLock, Timeout
    FILELOCK_OK = True
except Exception:
    FILELOCK_OK = False

# ========= Helpers: Files =========
def _load_json_safe(p: Path) -> dict:
    try:
        with p.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_json_atomic(p: Path, payload: dict) -> None:
    tmp = p.with_suffix(".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    tmp.replace(p)

# ========= Load KB / Aliases / Guards =========
KB = _load_json_safe(KB_PATH)
_ALIAS_MAP: Dict[str, str] = {}
if isinstance(KB.get("label_aliases"), dict):
    for canon, alist in KB["label_aliases"].items():
        for a in alist:
            _ALIAS_MAP[re.sub(r"\s+", " ", a.strip().lower())] = canon

AL2 = _load_json_safe(AL_PATH)
if isinstance(AL2.get("canonical"), dict):
    for a, canon in AL2["canonical"].items():
        _ALIAS_MAP[re.sub(r"\s+", " ", a.strip().lower())] = canon

KB_PRIORS: Dict[str, float] = dict(KB.get("priors") or {})
TRAIT_LINKS: Dict[str, Dict[str, float]] = dict(KB.get("trait_links") or {})
GUARDS: Dict[str, Any] = dict(KB.get("guards") or {})
HIGH_RISK_SPORTS: set = set(GUARDS.get("high_risk_sports", []) or [])
KB_ZI: Dict[str, Dict[str, List[str]]] = dict(KB.get("z_intent_keywords") or {})

_FORBIDDEN_GENERIC = set(
    KB.get("guards", {}).get("forbidden_generic_labels", [])
) | set(AL2.get("forbidden_generic", []) or [])

# ========= Arabic normalization =========
_AR_DIAC_RE = re.compile(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED\u0640]")
_AR_MAP = str.maketrans({
    "\u0623": "\u0627", "\u0625": "\u0627", "\u0622": "\u0627",
    "\u0624": "\u0648", "\u0626": "\u064A", "\u0629": "\u0647", "\u0649": "\u064A",
})
def _normalize_ar(t: str) -> str:
    if not t: return ""
    t = _AR_DIAC_RE.sub("", t)
    t = t.translate(_AR_MAP)
    t = re.sub(r"\s+", " ", t).strip()
    return t

# ========= Text normalizer =========
def _norm_text(val: Any) -> str:
    if val is None: return ""
    if isinstance(val, str): return val
    if isinstance(val, (list, tuple)):
        flat: List[str] = []
        for x in val:
            if isinstance(x, (list, tuple)):
                flat.extend(map(str, x))
            else:
                flat.append(str(x))
        return "ØŒ ".join([s.strip() for s in flat if s and str(s).strip()])
    if isinstance(val, dict):
        for k in ("text", "desc", "value", "answer"):
            if k in val and isinstance(val[k], str):
                return val[k]
        return json.dumps(val, ensure_ascii=False)
    return str(val)

# ========= Caches (soft) =========
try:
    from core.memory_cache import get_cached_personality, save_cached_personality
except Exception:
    _PERS_CACHE: Dict[str, str] = {}
    def get_cached_personality(analysis: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> Optional[str]:
        key = f"{lang}:{hash(json.dumps(analysis, ensure_ascii=False, sort_keys=True))}"
        return _PERS_CACHE.get(key)
    def save_cached_personality(analysis: Dict[str, Any], personality: str, lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> None:
        key = f"{lang}:{hash(json.dumps(analysis, ensure_ascii=False, sort_keys=True))}"
        _PERS_CACHE[key] = personality

try:
    from core.memory_cache import get_cached_recommendation, save_cached_recommendation  # type: ignore
except Exception:
    _REC_CACHE: Dict[str, List[str]] = {}
    def _answers_fingerprint(answers: Dict[str, Any], lang: str) -> str:
        try:
            blob = json.dumps(answers or {}, ensure_ascii=False, sort_keys=True)
        except Exception:
            blob = str(answers)
        h = hashlib.sha256((lang + "::" + blob).encode("utf-8")).hexdigest()[:24]
        return h
    def get_cached_recommendation(user_id: str, answers: Dict[str, Any], lang: str) -> Optional[List[str]]:
        key = f"{user_id}:{_answers_fingerprint(answers, lang)}"
        return _REC_CACHE.get(key)
    def save_cached_recommendation(user_id: str, answers: Dict[str, Any], lang: str, cards: List[str]) -> None:
        key = f"{user_id}:{_answers_fingerprint(answers, lang)}"
        _REC_CACHE[key] = list(cards or [])

# ========= User analysis imports =========
def _call_analyze_user_from_answers(user_id: str, answers: Dict[str, Any], lang: str) -> Dict[str, Any]:
    try:
        from analysis.user_analysis import analyze_user_from_answers as _ana
        try:
            out = _ana(user_id=user_id, answers=answers, lang=lang)
        except TypeError:
            out = _ana(answers)
        return {"traits": out} if isinstance(out, list) else (out or {})
    except Exception:
        try:
            from core.user_analysis import analyze_user_from_answers as _ana2
            try:
                out = _ana2(user_id=user_id, answers=answers, lang=lang)
            except TypeError:
                out = _ana2(answers)
            return {"traits": out} if isinstance(out, list) else (out or {})
        except Exception as e:
            _warn(f"user_analysis missing: {e}")
            return {"quick_profile": "fallback", "raw_answers": answers}

try:
    from core.layer_z_engine import analyze_silent_drivers_combined as analyze_silent_drivers
except Exception:
    try:
        from analysis.layer_z_engine import analyze_silent_drivers_combined as analyze_silent_drivers
    except Exception:
        def analyze_silent_drivers(answers: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> List[str]:
            return ["Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ù‚ØµÙŠØ±Ø©", "Ù†ÙÙˆØ± Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±", "ØªÙØ¶ÙŠÙ„ ØªØ­Ø¯Ù‘ÙŠ Ø°Ù‡Ù†ÙŠ"]

# ========= Signals & Intent =========
def _lang_key(lang: str) -> str:
    return "ar" if (lang or "").startswith("Ø§Ù„Ø¹") else "en"

def _extract_signals(answers: Dict[str, Any], lang: str) -> Dict[str, int]:
    """
    ÙŠØ¬Ù…Ø¹ ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø£ÙŠ Ù†ÙˆØ¹) ÙƒØ³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ© Ù…ÙˆØ­Ù‘Ø¯Ø© (AR-normalized + lowercase)
    Ø«Ù… ÙŠØ¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ø¯Ø§Ù„Ø© (Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ/Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ) Ø¨Ø­Ø³Ø¨ z_intent_keywords Ù…Ù† Ø§Ù„Ù€KB Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶Ø§Øª.
    """
    # 1) Ø·Ø¨ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ù‚ÙŠÙ… Ù„Ù†Øµ
    parts: List[str] = []
    for v in (answers or {}).values():
        vv = v.get("answer") if isinstance(v, dict) and "answer" in v else v
        parts.append(_norm_answer_value(vv))   # ÙŠØ­ÙˆÙ‘Ù„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¥Ù„Ù‰ "a, b, c" ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹

    blob = " ".join(parts) if parts else ""
    blob_l = blob.lower()
    blob_n = _normalize_ar(blob_l)  # ÙŠØ­Ø°Ù Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆÙŠÙˆØ­Ù‘Ø¯ Ø§Ù„Ø£Ù„Ù/Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©.. Ø¥Ù„Ø®

    res: Dict[str, int] = {}
    zi = KB_ZI.get(_lang_key(lang), {}) or {}

    # 2) Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…Ù† Ù‚Ø§Ø¦Ù…Ø©
    def any_kw(keys: List[str]) -> bool:
        if not keys: 
            return False
        for k in keys:
            k_norm = _normalize_ar(str(k).lower())
            if (k_norm and (k_norm in blob_n)) or (str(k).lower() in blob_l):
                return True
        return False

    # 3) Ù‚ÙˆØ§Ø¦Ù… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ØµØºÙŠØ±Ø© (Ù†Ø¶ÙŠÙ ØµÙŠØº Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©)
    vr_keys = zi.get("VR", []) + ["vr", "virtual reality", "headset", "ÙˆØ§Ù‚Ø¹ Ø§ÙØªØ±Ø§Ø¶ÙŠ", "Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ", "Ù†Ø¸Ø§Ø±Ù‡", "Ù†Ø¸Ø§Ø±Ø©"]
    prec_keys = zi.get("Ø¯Ù‚Ø©/ØªØµÙˆÙŠØ¨", []) + zi.get("Precision", []) + ["precision","aim","Ù†Ø´Ø§Ù†","Ø¯Ù‚Ù‡","ØªØµÙˆÙŠØ¨","Ø¯Ù‚Ø©"]
    stealth_keys = zi.get("ØªØ®ÙÙ‘ÙŠ", []) + zi.get("Stealth", []) + ["stealth","Ø¸Ù„","ØªØ®ÙÙŠ","Ø§Ø®ØªØ¨Ø§Ø¡"]
    combat_keys = zi.get("Ù‚ØªØ§Ù„ÙŠ", []) + zi.get("Combat", []) + ["Ù‚ØªØ§Ù„","Ù…Ø¨Ø§Ø±Ø²Ù‡","Ø§Ø´ØªØ¨Ø§Ùƒ","combat"]
    puzzles_keys = zi.get("Ø£Ù„ØºØ§Ø²/Ø®Ø¯Ø§Ø¹", []) + zi.get("Puzzles/Feint", []) + ["puzzle","Ù„ØºØ²","Ø£Ù„ØºØ§Ø²","Ø®Ø¯Ø¹Ù‡","Ø®Ø¯Ø§Ø¹"]
    solo_keys = zi.get("ÙØ±Ø¯ÙŠ", []) + ["solo","ÙˆØ­ÙŠØ¯","Ù„ÙˆØ­Ø¯ÙŠ","ÙØ±Ø¯ÙŠ"]
    team_keys = zi.get("Ø¬Ù…Ø§Ø¹ÙŠ", []) + ["team","group","ÙØ±ÙŠÙ‚","Ø¬Ù…Ø§Ø¹ÙŠ","ØªØ¹Ø§ÙˆÙ†ÙŠ"]
    calm_keys = zi.get("Ù‡Ø¯ÙˆØ¡/ØªÙ†ÙÙ‘Ø³", []) + zi.get("Calm/Breath", []) + ["breath","calm","Ù‡Ø¯ÙˆØ¡","ØªÙ†ÙØ³"]
    adren_keys = zi.get("Ø£Ø¯Ø±ÙŠÙ†Ø§Ù„ÙŠÙ†", []) + zi.get("Adrenaline", []) + ["fast","rush","Ø§Ù†Ø¯ÙØ§Ø¹","Ø£Ø¯Ø±ÙŠÙ†Ø§Ù„ÙŠÙ†"]

    # 4) Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    if any_kw(vr_keys):       res["vr"] = 1
    if any_kw(prec_keys):     res["precision"] = 1
    if any_kw(stealth_keys):  res["stealth"] = 1
    if any_kw(combat_keys):   res["combat"] = 1
    if any_kw(puzzles_keys):  res["puzzles"] = 1
    if any_kw(solo_keys):     res["solo_pref"] = 1
    if any_kw(team_keys):     res["team_pref"] = 1
    if any_kw(calm_keys):     res["breath"] = 1; res["calm"] = 1
    if any_kw(adren_keys):    res["high_agg"] = 1

    return res

def _call_analyze_intent(answers: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> List[str]:
    # Ø¬Ø±Ù‘Ø¨ ØªÙ†ÙÙŠØ° analyze_user_intent Ù…Ù† Ø£ÙŠ Ø·Ø¨Ù‚Ø© Ù…ØªÙˆÙØ±Ø©
    for modpath in ("core.layer_z_engine", "analysis.layer_z_engine"):
        try:
            mod = importlib.import_module(modpath)
            if hasattr(mod, "analyze_user_intent"):
                return list(mod.analyze_user_intent(answers, lang=lang) or [])
        except Exception:
            pass

    # fallback Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª
    intents = set()
    sig = _extract_signals(answers, lang)
    if sig.get("vr"):         intents.add("VR")
    if sig.get("stealth"):    intents.add("ØªØ®ÙÙ‘ÙŠ")
    if sig.get("puzzles"):    intents.add("Ø£Ù„ØºØ§Ø²/Ø®Ø¯Ø§Ø¹")
    if sig.get("precision"):  intents.add("Ø¯Ù‚Ø©/ØªØµÙˆÙŠØ¨")
    if sig.get("combat"):     intents.add("Ù‚ØªØ§Ù„ÙŠ")
    if sig.get("solo_pref"):  intents.add("ÙØ±Ø¯ÙŠ")
    if sig.get("team_pref"):  intents.add("Ø¬Ù…Ø§Ø¹ÙŠ")
    if sig.get("breath"):     intents.add("Ù‡Ø¯ÙˆØ¡/ØªÙ†ÙÙ‘Ø³")
    if sig.get("high_agg"):   intents.add("Ø£Ø¯Ø±ÙŠÙ†Ø§Ù„ÙŠÙ†")
    return list(intents)
    
# ========= Optional encoder =========
def _extract_profile(answers: Dict[str, Any], lang: str) -> Optional[Dict[str, Any]]:
    prof = answers.get("profile") if isinstance(answers, dict) else None
    if isinstance(prof, dict): return prof
    encode_answers = None
    try:
        from core.answers_encoder import encode_answers as _enc
        encode_answers = _enc
    except Exception:
        try:
            from analysis.answers_encoder import encode_answers as _enc
            encode_answers = _enc
        except Exception:
            encode_answers = None
    if encode_answers is None:
        return None
    try:
        enc = encode_answers(answers, lang=lang)
        preferences = enc.get("prefs", enc.get("preferences", {}))
        z_markers = enc.get("z_markers", [])
        signals   = enc.get("signals", [])
        hints = " | ".join([*z_markers, *signals])[:1000]
        return {
            "scores": enc.get("scores", {}),
            "axes": enc.get("axes", {}),
            "preferences": preferences,
            "hints_for_prompt": hints,
            "vr_inclination": enc.get("vr_inclination", 0),
            "confidence": enc.get("confidence", 0.0),
            "signals": signals,
            "z_markers": z_markers
        }
    except Exception as e:
        _warn(f"answers_encoder failed: {e}")
        return None

# ========= Evidence Gate (fallback) =========
def _norm_answer_value(v: Any) -> str:
    if v is None: return ""
    if isinstance(v, dict):
        if "answer" in v: return str(v.get("answer") or "")
        if "value" in v: return str(v.get("value") or "")
        return json.dumps(v, ensure_ascii=False)
    if isinstance(v, list):
        return ", ".join(map(str, v))
    return str(v)

def _egate_fallback(answers: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> Dict[str, Any]:
    if not isinstance(answers, dict) or not answers:
        status = "fail"; total_chars = 0; answered = 0
    else:
        vals = [_norm_answer_value(v) for v in answers.values() if str(v).strip()]
        total_chars = sum(len(s.strip()) for s in vals)
        answered = sum(1 for s in vals if len(s.strip()) >= 3)
        if _EG_REQUIRED_KEYS:
            if any(not _norm_answer_value(answers.get(k, "")).strip() for k in _EG_REQUIRED_KEYS):
                status = "fail"
            else:
                status = "pass" if (answered >= _EG_MIN_ANSWERS and total_chars >= _EG_MIN_TOTAL_CHARS) else "borderline"
        else:
            if answered == 0 or total_chars < 40:
                status = "fail"
            elif answered < _EG_MIN_ANSWERS or total_chars < _EG_MIN_TOTAL_CHARS:
                status = "borderline"
            else:
                status = "pass"

    followups = (
        [
            "ØªÙØ¶Ù‘Ù„ Ø§Ù„Ù„Ø¹Ø¨: ÙØ±Ø¯ÙŠ Ø£Ù… Ø¬Ù…Ø§Ø¹ÙŠØŸ ÙˆÙ„Ù…Ø§Ø°Ø§ Ø¨Ø³Ø·Ø± ÙˆØ§Ø­Ø¯.",
            "ØªÙ…ÙŠÙ„ Ù„Ù‡Ø¯ÙˆØ¡ ÙˆØ§Ù†Ø³ÙŠØ§Ø¨ Ø£Ù… Ø£Ø¯Ø±ÙŠÙ†Ø§Ù„ÙŠÙ† ÙˆÙ‚Ø±Ø§Ø±Ø§Øª Ø®Ø§Ø·ÙØ©ØŸ",
            "ØªØ­Ø¨ Ø¯Ù‚Ù‘Ø©/ØªØµÙˆÙŠØ¨ Ø£Ù… Ø£Ù„ØºØ§Ø²/Ø®Ø¯Ø§Ø¹ Ø¨ØµØ±ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø±ÙƒØ©ØŸ"
        ] if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else [
            "Do you prefer solo or team play â€” and why, in one short line?",
            "Do you want calm/flow or adrenaline/snap decisions?",
            "Are you more into precision/aim or puzzles/visual feints in motion?"
        ]
    )
    return {
        "status": "fail" if answered == 0 or total_chars < 40 else status,
        "answered": int(answered),
        "total_chars": int(total_chars),
        "required_missing": [k for k in _EG_REQUIRED_KEYS if not _norm_answer_value((answers or {}).get(k, "")).strip()],
        "followup_questions": followups[:3]
    }

def _run_egate(answers: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> Dict[str, Any]:
    if callable(egate_evaluate):
        try:
            res = egate_evaluate(answers=answers, lang=lang, cfg=EGCFG)
            if isinstance(res, dict) and "status" in res:
                return res
        except Exception as e:
            _warn(f"egate external failed: {e}")
    return _egate_fallback(answers, lang=lang)

def _format_followup_card(followups: List[str], lang: str) -> str:
    head = "ðŸ§­ Ù†Ø­ØªØ§Ø¬ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆØµÙŠØ©" if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "ðŸ§­ I need a few quick answers first"
    tips = "Ø§ÙƒØªØ¨ Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„." if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "One short line per question."
    lines = [head, "", tips, ""]
    for q in followups: lines.append(f"- {q}")
    lines.append("")
    lines.append("Ø£Ø±Ø³Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙˆØ³Ù†Ù‚ØªØ±Ø­ Ù‡ÙˆÙŠØ© Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙÙˆØ±Ù‹Ø§." if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
                 else "Send your answers and Iâ€™ll propose a clear sport-identity right away.")
    return "\n".join(lines)

# ========= Rules & helpers =========
_BLOCKLIST_AR = r"(Ø¬Ø±ÙŠ|Ø±ÙƒØ¶|Ø³Ø¨Ø§Ø­Ø©|ÙƒØ±Ø©|Ù‚Ø¯Ù…|Ø³Ù„Ø©|Ø·Ø§Ø¦Ø±Ø©|ØªÙ†Ø³|Ù…Ù„Ø§ÙƒÙ…Ø©|ÙƒØ§Ø±Ø§ØªÙŠÙ‡|ÙƒÙˆÙ†Øº ÙÙˆ|ÙŠÙˆØ¬Ø§|ÙŠÙˆØºØ§|Ø¨ÙŠÙ„Ø§ØªØ³|Ø±ÙØ¹|Ø£Ø«Ù‚Ø§Ù„|ØªØ²Ù„Ø¬|Ø¯Ø±Ø§Ø¬|Ø¯Ø±Ø§Ø¬Ø©|Ø±ÙƒÙˆØ¨|Ø®ÙŠÙˆÙ„|Ø¨Ø§Ø±ÙƒÙˆØ±|Ø¬ÙˆØ¯Ùˆ|Ø³ÙƒÙˆØ§Ø´|Ø¨Ù„ÙŠØ§Ø±Ø¯Ùˆ|Ø¬ÙˆÙ„Ù|ÙƒØ±Ø© Ø·Ø§Ø¦Ø±Ø©|ÙƒØ±Ø© Ø§Ù„ÙŠØ¯|Ù‡ÙˆÙƒÙŠ|Ø³Ø¨Ø§Ù‚|Ù…Ø§Ø±Ø§Ø«ÙˆÙ†|Ù…ØµØ§Ø±Ø¹Ø©)"
_BLOCKLIST_EN = r"(MMA|Boxing|Karate|Judo|Taekwondo|Soccer|Football|Basketball|Tennis|Swim|Swimming|Running|Run|Cycle|Cycling|Bike|Biking|Yoga|Pilates|Rowing|Row|Skate|Skating|Ski|Skiing|Climb|Climbing|Surf|Surfing|Golf|Volleyball|Handball|Hockey|Parkour|Wrestling)"
_name_re = re.compile(_BLOCKLIST_AR + "|" + _BLOCKLIST_EN, re.IGNORECASE)

_AVOID_GENERIC = [
    "Ø£ÙŠ Ù†Ø´Ø§Ø· Ø¨Ø¯Ù†ÙŠ Ù…ÙÙŠØ¯","Ø§Ø®ØªØ± Ù…Ø§ ÙŠÙ†Ø§Ø³Ø¨Ùƒ","Ø§Ø¨Ø¯Ø£ Ø¨Ø£ÙŠ Ø´ÙŠØ¡","Ø¬Ø±Ù‘Ø¨ Ø£ÙƒØ«Ø± Ù…Ù† Ø®ÙŠØ§Ø±",
    "Ù„Ø§ ÙŠÙ‡Ù… Ø§Ù„Ù†ÙˆØ¹","ØªØ­Ø±Ùƒ ÙÙ‚Ø·","Ù†Ø´Ø§Ø· Ø¹Ø§Ù…","Ø±ÙŠØ§Ø¶Ø© Ø¹Ø§Ù…Ø©","Ø£Ù†Øª ØªØ¹Ø±Ù Ù…Ø§ ÙŠÙ†Ø§Ø³Ø¨Ùƒ"
]
_SENSORY = [
    "ØªÙ†ÙÙ‘Ø³","Ø¥ÙŠÙ‚Ø§Ø¹","ØªÙˆØªØ±","Ø§Ø³ØªØ±Ø®Ø§Ø¡","Ø¯ÙØ¡","Ø¨Ø±ÙˆØ¯Ø©","ØªÙˆØ§Ø²Ù†","Ù†Ø¨Ø¶",
    "ØªØ¹Ø±Ù‘Ù‚","Ø´Ø¯Ù‘","Ù…Ø±ÙˆÙ†Ø©","Ù‡Ø¯ÙˆØ¡","ØªØ±ÙƒÙŠØ²","ØªØ¯ÙÙ‘Ù‚","Ø§Ù†Ø³Ø¬Ø§Ù…","Ø«ÙÙ‚Ù„","Ø®ÙÙÙ‘Ø©",
    "Ø¥Ø­Ø³Ø§Ø³","Ø§Ù…ØªØ¯Ø§Ø¯","Ø­Ø±Ù‚ Ù„Ø·ÙŠÙ","ØµÙØ§Ø¡","ØªÙ…Ø§Ø³Ùƒ"
]
_GENERIC_LABEL_RE = re.compile(
    r"(impress\w*\s+compact|generic\s+(sport|identity)|basic\s+flow|simple\s+flow|body\s+flow)",
    re.IGNORECASE
)

def _split_sentences(text: str) -> List[str]:
    if not text: return []
    return [s.strip() for s in re.split(r'(?<=[.!ØŸ])\s+|[\nØŒ]+', text) if s.strip()]

_FORBIDDEN_SENT = re.compile(
    r"(\b\d+(\.\d+)?\s*(?:min|mins|minute|minutes|second|seconds|hour|hours|Ø¯Ù‚ÙŠÙ‚Ø©|Ø¯Ù‚Ø§Ø¦Ù‚|Ø«Ø§Ù†ÙŠØ©|Ø«ÙˆØ§Ù†ÙŠ|Ø³Ø§Ø¹Ø©|Ø³Ø§Ø¹Ø§Øª)\b|"
    r"(?:rep|reps|set|sets|ØªÙƒØ±Ø§Ø±|Ø¹Ø¯Ø©|Ø¹Ø¯Ø§Øª|Ø¬ÙˆÙ„Ø©|Ø¬ÙˆÙ„Ø§Øª|Ã—)|"
    r"(?:ØªÙƒÙ„ÙØ©|Ù…ÙŠØ²Ø§Ù†ÙŠØ©|cost|budget|Ø±ÙŠØ§Ù„|Ø¯ÙˆÙ„Ø§Ø±|\$|â‚¬)|"
    r"(?:Ø¨Ø§Ù„Ø¨ÙŠØª|ÙÙŠ\s*Ø§Ù„Ø¨ÙŠØª|Ù‚Ø±Ø¨\s*Ø§Ù„Ù…Ù†Ø²Ù„|Ø¨Ø§Ù„Ù…Ù†Ø²Ù„|ÙÙŠ\s*Ø§Ù„Ù†Ø§Ø¯ÙŠ|ÙÙŠ\s*Ø§Ù„Ø¬ÙŠÙ…|ØµØ§Ù„Ø©|Ù†Ø§Ø¯ÙŠ|Ø¬ÙŠÙ…|ØºØ±ÙØ©|Ø³Ø§Ø­Ø©|Ù…Ù„Ø¹Ø¨|Ø­Ø¯ÙŠÙ‚Ø©|Ø´Ø§Ø·Ø¦|"
    r"Ø·Ø¨ÙŠØ¹Ø©|Ø®Ø§Ø±Ø¬ÙŠ(?:Ø©)?|Ø¯Ø§Ø®Ù„(?:ÙŠ|ÙŠØ©)?|outdoor|indoor|park|beach|gym|studio))",
    re.IGNORECASE
)

def _contains_blocked_name(t: str) -> bool:
    if not t: return False
    return bool(_name_re.search(t)) or bool(_name_re.search(_normalize_ar(t)))

def _mask_names(t: str) -> str:
    if ALLOW_SPORT_NAMES:
        return t or ""
    s = t or ""
    s = _name_re.sub("â€”", s)
    if s == (t or "") and _contains_blocked_name(t):
        s = "â€”"
    return s

def _scrub_forbidden(text: str) -> str:
    if not text: return ""
    kept = [s for s in _split_sentences(text) if not _FORBIDDEN_SENT.search(_normalize_ar(s))]
    out = "ØŒ ".join(kept).strip(" .ØŒ")
    if not out:
        out = _FORBIDDEN_SENT.sub("", _normalize_ar(text))
        out = re.sub(r"\s{2,}", " ", out).strip(" .ØŒ")
        if not out:
            out = text
    return out

def _clip(s: str, n: int) -> str:
    if not s: return ""
    s = s.strip()
    return s if len(s) <= n else (s[: max(0, n - 1)] + "â€¦")

def _to_bullets(text_or_list: Any, max_items: int = 6) -> List[str]:
    out: List[str] = []
    def _flat_add(x: Any) -> None:
        if x is None: return
        if isinstance(x, (list, tuple, set)):
            for y in x: _flat_add(y)
            return
        if isinstance(x, dict):
            for k in ("text", "desc", "value", "answer", "label", "title"):
                if k in x and isinstance(x[k], str) and x[k].strip():
                    out.append(x[k].strip()); return
            out.append(json.dumps(x, ensure_ascii=False)); return
        s = _norm_text(x).strip()
        if s: out.append(s)
    _flat_add(text_or_list)
    if len(out) == 1 and isinstance(text_or_list, str):
        raw = re.split(r"[;\n\.ØŒ]+", out[0])
        out = [i.strip(" -â€¢\t ") for i in raw if i.strip()]
    out = out[:max_items]
    out = [str(i) for i in out if str(i).strip()]
    return out

# ========= Z-axes alignment =========
_AR_TOK = {
    "calm": ["Ù‡Ø¯ÙˆØ¡","ØªÙ†ÙÙ‘Ø³","Ø¨Ø·ÙŠØ¡","Ø§Ø³ØªØ±Ø®Ø§Ø¡","ØµÙØ§Ø¡","Ø³ÙƒÙˆÙ†"],
    "adren": ["Ø§Ù†Ø¯ÙØ§Ø¹","ÙƒÙ…ÙŠÙ†","Ø³Ø±ÙŠØ¹","Ø§Ù†Ù‚Ø¶Ø§Ø¶","Ø¥Ø«Ø§Ø±Ø©","Ù‚Ø±Ø§Ø± Ø®Ø§Ø·Ù"],
    "solo": ["ÙØ±Ø¯ÙŠ","Ù„ÙˆØ­Ø¯Ùƒ","Ø°Ø§ØªÙŠØ©"],
    "group": ["Ø¬Ù…Ø§Ø¹Ø©","Ø´Ø±ÙŠÙƒ","ÙØ±ÙŠÙ‚","ØªØ¹Ø§ÙˆÙ†"],
    "tech": ["ØªÙ‚Ù†ÙŠØ©","ØªÙØ§ØµÙŠÙ„","ØµÙ‚Ù„","Ø¯Ù‚Ø©","Ø¶Ø¨Ø·"],
    "intu": ["Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­Ø³Ø§Ø³","Ø­Ø¯Ø³","ØªÙ„Ù‚Ø§Ø¦ÙŠØ©","ØªØ¯ÙÙ‘Ù‚"]
}
_EN_TOK = {
    "calm": ["calm","breath","slow","relax","settle"],
    "adren": ["burst","fast","risk","strike","adrenaline"],
    "solo": ["solo","alone","individual"],
    "group": ["team","partner","group","co-op"],
    "tech": ["technique","detail","precision","drill","refine"],
    "intu": ["by feel","intuitive","flow","improvise"]
}
def _axes_expectations(axes: Dict[str, float], lang: str) -> Dict[str, List[str]]:
    tok = _AR_TOK if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else _EN_TOK
    out: Dict[str, List[str]] = {}
    if not isinstance(axes, dict): return out
    ca = axes.get("calm_adrenaline")
    if isinstance(ca, (int, float)):
        out["calm_adrenaline"] = tok["adren"] if ca >= 0.5 else tok["calm"] if ca <= -0.5 else []
    sg = axes.get("solo_group")
    if isinstance(sg, (int, float)):
        out["solo_group"] = tok["group"] if sg >= 0.5 else tok["solo"] if sg <= -0.5 else []
    ti = axes.get("tech_intuition")
    if isinstance(ti, (int, float)):
        out["tech_intuition"] = tok["intu"] if ti >= 0.5 else tok["tech"] if ti <= -0.5 else []
    return out

def _mismatch_with_axes(rec: Dict[str, Any], axes: Dict[str, float], lang: str) -> bool:
    exp = _axes_expectations(axes or {}, lang)
    if not exp: return False
    blob = " ".join(_norm_text(rec.get(k,"")) for k in ("what_it_looks_like","inner_sensation","why_you","first_week"))
    blob_l = blob.lower()
    for _, words in exp.items():
        if words and not any(w.lower() in blob_l for w in words):
            return True
    return False

# ========= Label normalization / similarity =========
def _canonical_label(label: str) -> str:
    if not label: return ""
    lab = re.sub(r"\s+", " ", label).strip(" -â€”:ØŒ").lower()
    lab = _normalize_ar(lab)
    return lab

def _label_is_generic(label: str) -> bool:
    # Ù†ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù€canonical ÙˆÙ…Ù† Ø£ÙŠ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø±ÙŠØ¬ÙŠÙƒØ³ Ø§Ù„Ù…Ø±Ù†
    if not label:
        return True
    lab = _canonical_label(label)
    if (lab in _GENERIC_LABELS) or (len(lab) <= 3):
        return True
    return bool(_GENERIC_LABEL_RE.search(label))

def _tokenize(text: str) -> List[str]:
    if not text: return []
    t = _normalize_ar(text.lower())
    toks = re.split(r"[^a-zA-Z0-9\u0600-\u06FF]+", t)
    return [w for w in toks if w and len(w) > 2]

def _sig_for_rec(r: Dict[str, Any]) -> set:
    core = r.get("core_skills") or []
    core_txt = " ".join(core) if isinstance(core, list) else str(core)
    toks = set(_tokenize(r.get("sport_label","")) + _tokenize(core_txt))
    return toks

def _jaccard(a: set, b: set) -> float:
    if not a and not b: return 1.0
    inter = len(a & b)
    union = len(a | b) or 1
    return inter / union

# ========= Sanitizers / Fallbacks =========
def _sanitize_record(r: Dict[str, Any]) -> Dict[str, Any]:
    r = dict(r or {})
    r.pop("practical_fit", None)
    for k in ("sport_label","scene","what_it_looks_like","inner_sensation","why_you",
              "first_week","progress_markers","win_condition","variant_vr","variant_no_vr","vr_idea","mode"):
        if k in r:
            s = _scrub_forbidden(_mask_names(_norm_text(r.get(k))))
            r[k] = s
    cs = r.get("core_skills")
    if isinstance(cs, str):
        parts = [p.strip(" -â€¢\t") for p in re.split(r"[,\nØŒ]+", cs) if p.strip()]
        r["core_skills"] = parts[:6]
    elif isinstance(cs, (list, tuple)):
        skills = [_norm_text(x).strip() for x in cs if _norm_text(x).strip()]
        r["core_skills"] = skills[:6]
    else:
        r["core_skills"] = []
    try:
        d = int(r.get("difficulty", 3))
        r["difficulty"] = max(1, min(5, d))
    except Exception:
        r["difficulty"] = 3
    if r.get("mode") not in ("Solo","Team","Solo/Team","ÙØ±Ø¯ÙŠ","Ø¬Ù…Ø§Ø¹ÙŠ","ÙØ±Ø¯ÙŠ/Ø¬Ù…Ø§Ø¹ÙŠ"):
        r["mode"] = r.get("mode","Solo")
    return r

# ========= Fallback identities (AR/EN) =========
def _fallback_identity(i: int, lang: str) -> Dict[str, Any]:
    if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
        presets = [
            {
                "sport_label":"Tactical Immersive Combat",
                "what_it_looks_like":"Ø³Ø§Ø­Ø© Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ØµØ±ÙŠØ© Ø£Ùˆ VR: ØªØªØ¨Ù‘Ø¹ØŒ ÙƒÙ…ÙŠÙ†ØŒ Ù‚Ø±Ø§Ø± Ø®Ø§Ø·ÙØŒ Ø§Ù‚ØªØ±Ø§Ø¨ Ù…Ø­Ø³ÙˆØ¨ØŒ Ø§Ù†Ø³Ø­Ø§Ø¨ Ø¢Ù…Ù†.",
                "inner_sensation":"Ø§Ù†Ø¯Ù…Ø§Ø¬ Ø°Ù‡Ù†ÙŠ Ù…Ø¹ ÙŠÙ‚Ø¸Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ«Ù‚Ø© Ù‡Ø§Ø¯Ø¦Ø©.",
                "why_you":"ØªÙƒØ±Ù‡ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØªÙØ¶Ù‘Ù„ ØµØ±Ø§Ø¹Ù‹Ø§ ØªÙƒØªÙŠÙƒÙŠÙ‹Ø§ ÙŠØ®ØªØ¨Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ ÙˆØ§Ù„Ø£Ø¹ØµØ§Ø¨.",
                "first_week":"Ø§Ø¨Ø¯Ø£ Ø¨Ø¬ÙˆÙ„Ø© Ø­Ø³Ù‘ÙŠØ©: ØªØ¹Ø±Ù‘Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ØŒ Ø¬Ø±Ù‘Ø¨ Ø§Ù‚ØªØ±Ø§Ø¨/Ø§Ù†Ø³Ø­Ø§Ø¨ØŒ ÙˆØ«Ø¨Ù‘Øª ØªÙ†ÙÙ‘Ø³Ùƒ Ù‚Ø¨Ù„ Ø§Ù„Ù‚Ø±Ø§Ø±.",
                "progress_markers":"Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ø³Ø±Ø¹ØŒ Ù‡Ø¯ÙˆØ¡ ØªØ­Øª Ø§Ù„Ø¶ØºØ·ØŒ Ø¥Ø­Ø³Ø§Ø³ Ø¨Ø³ÙŠØ·Ø±Ø© Ø£Ø¹Ù„Ù‰.",
                "win_condition":"Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù‡Ø¯Ù Ø¯ÙˆÙ† Ø§Ù†ÙƒØ´Ø§Ù Ø£Ùˆ ØªØ¹Ø·ÙŠÙ„ 'Ø§Ù„Ø®ØµÙ…' Ø¨Ù‚Ø±Ø§Ø± Ø®Ø§Ø·Ù.",
                "core_skills":["ØªØªØ¨Ù‘Ø¹ Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ©","ØªØºÙŠÙŠØ± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹","Ù‚Ø±Ø§Ø± Ø³Ø±ÙŠØ¹","Ø«Ø¨Ø§Øª Ø§Ù„ØªÙˆØ§Ø²Ù†","ØªÙ†ÙÙ‘Ø³ Ù‡Ø§Ø¯Ø¦"],
                "mode":"Solo/Team",
                "variant_vr":"Ù…Ø¨Ø§Ø±Ø²Ø§Øª ØªÙƒØªÙŠÙƒÙŠØ© ÙÙŠ VR Ù…Ø¹ ØªØªØ¨Ø¹ Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ© Ø§Ù„Ø®ØµÙ….",
                "variant_no_vr":"Ø¹Ù‚Ø¨Ø§Øª Ø®ÙÙŠÙØ© Ù…Ø¹ Ù…Ù…Ø±Ø§Øª Ø¸Ù„Ù‘.",
                "difficulty":3
            },
            {
                "sport_label":"Stealth-Flow Missions",
                "what_it_looks_like":"Ù…Ø³Ø§Ø± ØµØ§Ù…Øª Ù…ØªØ¯Ø±Ù‘Ø¬: Ø§Ø®ØªØ¨Ø§Ø¡ Ù‚ØµÙŠØ±ØŒ Ø¸Ù‡ÙˆØ± Ù…Ø­Ø³ÙˆØ¨ØŒ Ù„Ù…Ø³ 'Ø¹Ù„Ø§Ù…Ø©' Ø«Ù… Ø§Ø®ØªÙØ§Ø¡.",
                "inner_sensation":"ØªØ±ÙƒÙŠØ² Ø¹Ù…ÙŠÙ‚ ÙˆØªÙ†Ø¸ÙŠÙ… Ù„Ù„Ù†ÙØ³ Ù…Ø¹ Ø­Ø±ÙƒØ© Ù†Ø§Ø¹Ù…Ø©.",
                "why_you":"ØªØ¨ØºÙ‰ ØªÙ‚Ø¯Ù‘Ù… Ù…Ù„Ù…ÙˆØ³ Ø¨Ø¯ÙˆÙ† Ø¶Ø¬ÙŠØ¬ ÙˆØªØ­Ø¨ Ø§Ù„Ø¥ØªÙ‚Ø§Ù† Ø§Ù„Ù‡Ø§Ø¯Ø¦.",
                "first_week":"Ø¯Ø±Ù‘Ø¨ ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø±Ø¹Ø§Øª Ø¨Ø³Ù„Ø§Ø³Ø© ÙˆÙ…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ø§Ù„Ø¢Ù…Ù†Ø©.",
                "progress_markers":"ØªÙˆØªØ± Ø£Ù‚Ù„ØŒ Ù†Ø¹ÙˆÙ…Ø© Ø­Ø±ÙƒØ©ØŒ Ù‚Ø±Ø§Ø±Ø§Øª Ø£ÙˆØ¶Ø­.",
                "win_condition":"Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ù…Ù‡Ù…Ø© Ø¯ÙˆÙ† Ø§Ù†ÙƒØ´Ø§Ù.",
                "core_skills":["ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø¸Ù‡ÙˆØ±","Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø­ÙˆØ§Ø¬Ø²","ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹","ØªÙ†ÙÙ‘Ø³ ØµØ§Ù…Øª","ØªÙˆØ§Ø²Ù†"],
                "mode":"Solo",
                "variant_vr":"ØªØ³Ù„Ù„ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø¹ Ù…Ø¤Ø´Ù‘Ø± Ø§Ù†ÙƒØ´Ø§Ù Ø¨ØµØ±ÙŠ.",
                "variant_no_vr":"Ø¹ÙˆØ§Ø¦Ù‚ Ø®ÙÙŠÙØ© ÙˆØ£Ø´Ø±Ø·Ø© Ø¸Ù„.",
                "difficulty":2
            },
            {
                "sport_label":"Mind-Trap Puzzles in Motion",
                "what_it_looks_like":"Ø£Ù„ØºØ§Ø² Ù‚Ø±Ø§Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø±ÙƒØ©: ØªØ­ÙˆÙŠÙ„ Ù…Ø³Ø§Ø±ØŒ Ø®Ø¯Ø¹Ø© Ø¨ØµØ±ÙŠØ©ØŒ Ø­Ø±ÙƒØ© Ù…Ø¶Ø§Ø¯Ø© Ù„Ø­Ø±ÙƒØ© 'Ø®ØµÙ…' Ø§ÙØªØ±Ø§Ø¶ÙŠ.",
                "inner_sensation":"ÙØ¶ÙˆÙ„ Ø°Ù‡Ù†ÙŠ Ù…Ø¹ Ù„Ø°Ù‘Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù.",
                "why_you":"ØªØ­Ø¨ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆÙ…Ø¨Ø§Ø±Ø²Ø© Ø§Ù„Ù‡ÙˆÙŠØ© Ø°Ù‡Ù†ÙŠÙ‹Ø§ Ù‚Ø¨Ù„ Ø¬Ø³Ø¯ÙŠÙ‹Ø§.",
                "first_week":"Ø­Ù„ Ù„ØºØ² Ø­Ø±ÙƒÙŠ Ø¨Ø³ÙŠØ· Ù…Ø¹ ØªØªØ¨Ù‘Ø¹ Ø§Ù„Ù†ÙØ³ØŒ Ø«Ù… Ø£Ø¶Ù Ø®Ø¯Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©.",
                "progress_markers":"ÙˆØ¶ÙˆØ­ ØªØ±ÙƒÙŠØ²ØŒ Ø«Ù‚Ø© Ù‚Ø±Ø§Ø±ØŒ ØªÙ†Ø§ØºÙ… Ø­Ø±ÙƒØ©/ÙÙƒØ±.",
                "win_condition":"Ø­Ù„ Ø§Ù„Ù„ØºØ² Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡ Ù…ØªØªØ§Ù„ÙŠØ©.",
                "core_skills":["Ø®Ø¯Ø§Ø¹ Ø¨ØµØ±ÙŠ","ØªØ­ÙˆÙŠÙ„ Ù…Ø³Ø§Ø±","ØªØ«Ø¨ÙŠØª Ù†Ø¸Ø±Ø©","Ù‚Ø±Ø§Ø± Ø¨Ø¯ÙŠÙ‡ÙŠ","Ù‡Ø¯ÙˆØ¡ ØªØ­Øª Ø¶ØºØ·"],
                "mode":"Solo",
                "variant_vr":"Ø£ÙØ®Ø§Ø® Ø¨ØµØ±ÙŠØ© ØªÙØ§Ø¹Ù„ÙŠØ©.",
                "variant_no_vr":"Ù…Ø³Ø§Ø±Ø§Øª Ø£Ø±Ø¶ÙŠØ© Ø°Ø§Øª Ø¥Ø´Ø§Ø±Ø§Øª Ù…Ø®ÙÙŠØ©.",
                "difficulty":2
            },
            {
                "sport_label":"Range Precision Circuit",
                "what_it_looks_like":"Ø¯Ù‚Ø© ØªØµÙˆÙŠØ¨ Ù‡Ø§Ø¯Ø¦Ø© Ù…Ø¹ Ø²ÙˆØ§ÙŠØ§ Ø«Ø§Ø¨ØªØ© ÙˆØ§Ù†ØªÙ‚Ø§Ù„ Ù…Ù†Ø¸Ù‘Ù… Ø¨ÙŠÙ† Ø£Ù‡Ø¯Ø§Ù.",
                "inner_sensation":"Ù‡Ø¯ÙˆØ¡ Ù…ØªÙ…Ø±ÙƒØ² ÙˆÙ†Ø¨Ø¶ Ù…Ø³ØªÙ‚Ø±.",
                "why_you":"ØªÙ…ÙŠÙ„ Ù„ØªÙ‡Ø°ÙŠØ¨ Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„Ø§ØªÙ‚Ø§Ù† Ø§Ù„ØµØ§Ù…Øª.",
                "first_week":"Ø«Ø¨Ù‘Øª Ø§Ù„Ù†Ø¸Ø±Ø©ØŒ Ø§Ø¶Ø¨Ø· Ø§Ù„Ù†ÙØ³ØŒ Ø¨Ø¯Ù‘Ù„ Ø²ÙˆØ§ÙŠØ§Ùƒ Ø¨Ø³Ù„Ø§Ø³Ø©.",
                "progress_markers":"Ø«Ø¨Ø§Øª ÙŠØ¯ØŒ Ù‚Ø±Ø§Ø± Ø£ÙˆØ¶Ø­ØŒ ØªÙˆØªØ± Ø£Ù‚Ù„.",
                "win_condition":"ØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø«Ø§Ø¨ØªØ© Ø¹Ø¨Ø± Ø³Ù„Ø³Ù„Ø© Ø£Ù‡Ø¯Ø§Ù Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡ Ù…ØªØªØ§Ù„ÙŠØ©.",
                "core_skills":["ØªØ«Ø¨ÙŠØª Ù†Ø¸Ø±Ø©","Ø¶Ø¨Ø· Ù†ÙØ³","Ø§Ù†ØªÙ‚Ø§Ù„ Ø²ÙˆØ§ÙŠØ§","ØªØ­ÙƒÙ‘Ù… Ø¯Ù‚ÙŠÙ‚"],
                "mode":"Solo",
                "variant_vr":"ØªØµÙˆÙŠØ¨ Ø§ÙØªØ±Ø§Ø¶ÙŠ ØªÙØ§Ø¹Ù„ÙŠ.",
                "variant_no_vr":"Ù„ÙˆØ­Ø§Øª Ø£Ù‡Ø¯Ø§Ù Ø¥Ø³ÙÙ†Ø¬ÙŠØ© Ø¢Ù…Ù†Ø©.",
                "difficulty":2
            },
            {
                "sport_label":"Grip & Balance Ascent",
                "what_it_looks_like":"Ù…Ø³Ø§Ø± Ù‚Ø¨Ø¶Ø§Øª ÙˆØªÙˆØ§Ø²Ù† ØªØ¯Ø±ÙŠØ¬ÙŠØŒ Ù‚Ø±Ø§Ø¡Ø© Ù…Ø³ÙƒØŒ ØªØ­ÙˆÙŠÙ„ ÙˆØ²Ù† Ù‡Ø§Ø¯Ø¦.",
                "inner_sensation":"ØªÙ…Ø§Ø³Ùƒ Ø¯Ø§Ø®Ù„ÙŠ ÙˆØ«Ù‚Ø© Ø­Ø±ÙƒØ©.",
                "why_you":"ØªØ­Ø¨ ØªØ­Ø¯Ù‘ÙŠ ØªØ­ÙƒÙ‘Ù… Ø§Ù„Ø¬Ø³Ø¯ Ø¨Ø¯ÙˆÙ† Ø¶Ø¬ÙŠØ¬.",
                "first_week":"Ø§Ù‚Ø±Ø£ Ù…ÙˆØ¶Ø¹ Ø§Ù„Ù‚Ø¨Ø¶Ø©ØŒ ÙˆØ²Ù‘Ù† Ø§Ù„Ø­Ù…Ù„ØŒ ØªØ­Ø±Ù‘Ùƒ Ø¨Ø¨Ø·Ø¡ ØµØ§Ø¹Ø¯.",
                "progress_markers":"ØªØ¹Ø¨ Ø£Ù‚Ù„ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø¯ØŒ Ø§ØªØ²Ø§Ù† Ø£ÙˆØ¶Ø­.",
                "win_condition":"Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø¹Ù„ÙˆÙŠØ© Ø¯ÙˆÙ† Ø¥ÙÙ„Ø§Øª.",
                "core_skills":["Ù‚Ø¨Ø¶Ø©","ØªØ­ÙˆÙŠÙ„ ÙˆØ²Ù†","ØªÙˆØ§Ø²Ù†","Ù‚Ø±Ø§Ø¡Ø© Ù…Ø³Ø§Ø±"],
                "mode":"Solo",
                "variant_vr":"Ù…Ø³Ø§Ø±Ø§Øª Ù‚Ø¨Ø¶Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.",
                "variant_no_vr":"Ø¹Ù†Ø§ØµØ± Ù‚Ø¨Ø¶Ø© Ø¢Ù…Ù†Ø© Ø®ÙÙŠÙØ©.",
                "difficulty":2
            }
        ]
    else:
        presets = [
            {
                "sport_label":"Tactical Immersive Combat",
                "what_it_looks_like":"Arena/VR with tracking, ambush, snap decisions, calculated approach and safe exit.",
                "inner_sensation":"Locked-in focus with calm confidence.",
                "why_you":"You dislike repetition and enjoy tactical mind-body duels.",
                "first_week":"Feel the rhythm, practice approach/retreat, anchor breath before decisions.",
                "progress_markers":"Faster decisions, calmer under pressure, stronger control.",
                "win_condition":"Reach objective unseen or neutralize threat via snap decision.",
                "core_skills":["angle tracking","tempo shifts","fast decision","balance","quiet breath"],
                "mode":"Solo/Team",
                "variant_vr":"Tactical VR duels with FOV tracking.",
                "variant_no_vr":"Foam obstacles with shadow lanes.",
                "difficulty":3
            },
            {
                "sport_label":"Stealth-Flow Missions",
                "what_it_looks_like":"Silent path: brief hide, measured reveal, tag and vanish.",
                "inner_sensation":"Deep focus with smooth breathing.",
                "why_you":"You want tangible progress without noise and love quiet mastery.",
                "first_week":"Practice smooth tempo shifts and safe angles.",
                "progress_markers":"Lower tension, smoother movement, clearer decisions.",
                "win_condition":"Complete the mission without detection.",
                "core_skills":["timed reveal","reading cover","tempo control","silent breath","balance"],
                "mode":"Solo",
                "variant_vr":"Stealth VR with exposure indicator.",
                "variant_no_vr":"Light obstacles and shadow tapes.",
                "difficulty":2
            },
            {
                "sport_label":"Mind-Trap Puzzles in Motion",
                "what_it_looks_like":"Decision puzzles in motion: route switch, visual feint, counter-move.",
                "inner_sensation":"Curious mind with discovery joy.",
                "why_you":"You enjoy deep understanding and identity duelsâ€”mind first.",
                "first_week":"Solve a simple motion puzzle with breath tracking; add one feint.",
                "progress_markers":"Sharper focus, steadier decisions, mindâ€“body sync.",
                "win_condition":"Solve without consecutive errors.",
                "core_skills":["visual feint","path switch","gaze hold","intuitive decision","calm under stress"],
                "mode":"Solo",
                "variant_vr":"Interactive visual traps.",
                "variant_no_vr":"Floor cues with hidden hints.",
                "difficulty":2
            }
        ]
    return presets[i % len(presets)]

def _fill_defaults(r: Dict[str, Any], lang: str) -> Dict[str, Any]:
    r = dict(r or {})
    if not r.get("win_condition"):
        r["win_condition"] = ("Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ù…Ù‡Ù…Ø© Ø¯ÙˆÙ† Ø§Ù†ÙƒØ´Ø§Ù." if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
                              else "Complete the mission without being detected.")
    if not r.get("core_skills"):
        r["core_skills"] = ["ØªØªØ¨Ù‘Ø¹ Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø±Ø¤ÙŠØ©","ØªØºÙŠÙŠØ± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹","Ù‚Ø±Ø§Ø± Ø³Ø±ÙŠØ¹"] if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" \
                           else ["angle tracking","tempo shifts","fast decision"]
    if not r.get("mode"):
        r["mode"] = "Solo/Team"
    if not r.get("variant_vr"):
        r["variant_vr"] = ("Ù…Ø¨Ø§Ø±Ø²Ø§Øª ØªÙƒØªÙŠÙƒÙŠØ© ÙÙŠ VR Ù…Ø¹ Ù…Ø¤Ø´Ù‘Ø± Ù…Ø¬Ø§Ù„ Ø±Ø¤ÙŠØ©." if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
                           else "Tactical VR duels with FOV indicator.")
    if not r.get("variant_no_vr"):
        r["variant_no_vr"] = ("Ø¹Ù‚Ø¨Ø§Øª Ø®ÙÙŠÙØ© Ù…Ø¹ Ù…Ù…Ø±Ø§Øª Ø¸Ù„." if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
                              else "Light obstacle arena with shadow lanes.")
    return r

# ======== HARD DEDUPE (no repeats, no near-duplicates) ========
def _hard_dedupe_and_fill(recs: List[Dict[str, Any]], lang: str) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    used_labels = set()
    used_sigs: List[set] = []

    def pick_unique_fallback(idx: int) -> Dict[str, Any]:
        for j in range(idx, idx+6):
            fb = _fallback_identity(j, lang)
            lab = _canonical_label(fb.get("sport_label",""))
            sig = _sig_for_rec(fb)
            if lab and (lab not in used_labels) and all(_jaccard(sig, s) <= 0.6 for s in used_sigs):
                return fb
        fb = _fallback_identity(idx, lang)
        fb["sport_label"] = (fb.get("sport_label","") + " â€” Alt")
        return fb

    for i in range(3):
        r = recs[i] if i < len(recs) else {}
        r = _fill_defaults(_sanitize_record(r), lang)

        lab = _canonical_label(r.get("sport_label",""))
        if not lab or _label_is_generic(lab):
            r = pick_unique_fallback(i); lab = _canonical_label(r.get("sport_label",""))

        if lab in used_labels:
            r = pick_unique_fallback(i); lab = _canonical_label(r.get("sport_label",""))

        sig = _sig_for_rec(r)
        if any(_jaccard(sig, s) > 0.6 for s in used_sigs):
            r = pick_unique_fallback(i); lab = _canonical_label(r.get("sport_label","")); sig = _sig_for_rec(r)

        out.append(r)
        used_labels.add(lab)
        used_sigs.append(sig)

    return out

# ========= Traits & Scoring =========
def _derive_binary_traits(analysis: Dict[str, Any], answers: Dict[str, Any], lang: str) -> Dict[str, float]:
    traits: Dict[str, float] = {}
    prof = (analysis or {}).get("encoded_profile") or {}
    axes = (prof or {}).get("axes") or {}
    silent = set(map(str, (analysis or {}).get("silent_drivers") or []))
    sig = _extract_signals(answers, lang)

    sg = float(axes.get("solo_group", 0.0)) if isinstance(axes, dict) else 0.0
    if sg <= -0.35: traits["introvert"] = 1.0; traits["prefers_solo"] = 1.0
    if sg >=  0.35: traits["extrovert"] = 1.0; traits["prefers_team"] = 1.0
    if sig.get("solo_pref"): traits["prefers_solo"] = max(1.0, traits.get("prefers_solo", 0))
    if sig.get("team_pref"): traits["prefers_team"] = max(1.0, traits.get("prefers_team", 0))

    ca = float(axes.get("calm_adrenaline", 0.0)) if isinstance(axes, dict) else 0.0
    if ca <= -0.35 or sig.get("breath"):
        traits["calm_regulation"] = max(traits.get("calm_regulation", 0.0), 0.8)
    if ca >= 0.35 or sig.get("high_agg"):
        traits["sensation_seeking"] = max(traits.get("sensation_seeking", 0.0), 0.8)

    ti_val = axes.get("tech_intuition", 0.0)
    ti = float(ti_val) if isinstance(ti_val, (int, float)) else 0.0
    if ti <= -0.35 or sig.get("precision"):
        traits["precision"] = max(traits.get("precision", 0.0), 0.8)

    if sig.get("stealth"):
        traits["tactical_mindset"] = max(1.0, traits.get("tactical_mindset", 0))
    if sig.get("puzzles"): traits["likes_puzzles"] = 1.0
    if sig.get("combat"): traits["tactical_mindset"] = max(1.0, traits.get("tactical_mindset", 0))
    vr_i = float((prof or {}).get("vr_inclination", 0.0))
    if sig.get("vr") or vr_i >= 0.4:
        traits["vr_inclination"] = max(traits.get("vr_inclination", 0.0), max(vr_i, 0.8))

    if traits.get("calm_regulation", 0) >= 0.8 or traits.get("likes_puzzles", 0) >= 1.0:
        traits["sustained_attention"] = max(traits.get("sustained_attention", 0.0), 0.6)

    ar_blob = _normalize_ar(" ".join(_norm_answer_value(v) for v in (answers or {}).values()).lower())
    if any(w in ar_blob for w in ["Ù‚Ù„Ù‚","Ù…Ø®Ø§ÙˆÙ","ØªÙˆØªØ± Ø´Ø¯ÙŠØ¯","Ø±Ù‡Ø§Ø¨","Ø®ÙˆÙ"]):
        traits["anxious"] = 1.0
    if any("Ù†ÙÙˆØ±" in s or "ØªÙƒØ±Ø§Ø±" in s for s in silent):
        traits["low_repetition_tolerance"] = 1.0
    if any("Ø§Ù†Ø¬Ø§Ø²Ø§Øª Ù‚ØµÙŠØ±Ù‡" in _normalize_ar(s) for s in silent):
        traits["needs_quick_wins"] = 1.0

    return traits

def _score_candidates_from_links(traits: Dict[str, float]) -> List[Tuple[float, str]]:
    anxious = traits.get("anxious", 0.0) >= 0.8 and GUARDS.get("no_high_risk_for_anxiety", True)
    labels = set(KB_PRIORS.keys())
    for t, mapping in (TRAIT_LINKS or {}).items():
        labels.update(mapping.keys())
    scored: List[Tuple[float, str]] = []
    for label in labels:
        if anxious and label in HIGH_RISK_SPORTS:
            scored.append((-1e9, label)); continue
        s = float(KB_PRIORS.get(label, 0.0))
        for t_name, strength in traits.items():
            link = (TRAIT_LINKS.get(t_name) or {}).get(label, 0.0)
            if link: s += float(strength) * float(link)
        scored.append((s, label))
    scored.sort(key=lambda x: x[0], reverse=True)
    return scored

# ========= Templates by label (AR/EN) =========
def _canon_label(label: str) -> str:
    lab = _normalize_ar((label or "")).lower().strip(" -â€”:ØŒ")
    if not lab: return ""
    if lab in _ALIAS_MAP: return _ALIAS_MAP[lab]
    lab = re.sub(r"[^a-z0-9\u0600-\u06FF]+", " ", lab)
    lab = re.sub(r"\s+", " ", lab).strip()
    return lab

def _is_forbidden_generic(label: str) -> bool:
    base = _normalize_ar((label or "")).lower()
    return any(g in base for g in _FORBIDDEN_GENERIC)

def _template_for_label(label: str, lang: str) -> Optional[Dict[str, Any]]:
    L = _canon_label(label)
    KB_PRESETS = {
        "tactical_immersive_combat": _fallback_identity(0, lang),
        "stealth_flow_missions": _fallback_identity(1, lang),
        "mind_trap_puzzles": _fallback_identity(2, lang),
        "range_precision_circuit": _fallback_identity(3, lang),
        "grip_balance_ascent": _fallback_identity(4, lang),
    }
    if L in KB_PRESETS:
        return _sanitize_record(_fill_defaults(KB_PRESETS[L], lang))
    return None

# ====== Blacklist (persistent, JSON) =========================================
def _load_blacklist() -> dict:
    if FILELOCK_OK:
        try:
            with FileLock(str(BL_PATH) + ".lock", timeout=3):
                bl = _load_json_safe(BL_PATH)
        except Timeout:
            _warn("blacklist lock timeout on read; proceeding without lock")
            bl = _load_json_safe(BL_PATH)
    else:
        time.sleep(0.01 + random.random()*0.02)
        bl = _load_json_safe(BL_PATH)

    if not isinstance(bl.get("labels"), dict):
        bl["labels"] = {}
    bl.setdefault("version", "1.0")
    return bl

def _bl_has(bl: dict, label: str) -> bool:
    c = _canon_label(label)
    return bool(c and c in bl["labels"])

def _bl_add(bl: dict, label: str, alias: str = "") -> None:
    c = _canon_label(label)
    if not c: return
    rec = bl["labels"].get(c, {"aliases": [], "count": 0, "first_seen": None})
    if alias and alias not in rec["aliases"]:
        rec["aliases"].append(alias)
    rec["count"] = int(rec.get("count", 0)) + 1
    if not rec.get("first_seen"):
        rec["first_seen"] = datetime.utcnow().isoformat() + "Z"
    rec["last_used"] = datetime.utcnow().isoformat() + "Z"
    bl["labels"][c] = rec

_AR_VARIANTS = [
    "Ù†Ø³Ø®Ø© Ø¸Ù„Ù‘ÙŠØ©","Ù†Ù…Ø· Ù…Ø±Ø§ÙˆØºØ©","Ø®Ø· ØªØ­ÙƒÙ‘Ù… Ù‡Ø§Ø¯Ø¦","Ù†Ø³Ø®Ø© ØªÙÙƒÙŠÙƒ ØªÙƒØªÙŠÙƒÙŠ",
    "Ù…Ù†Ø§ÙˆØ±Ø© Ø¹ÙƒØ³ÙŠØ©","Ø²Ø§ÙˆÙŠØ© ØµØ§Ù…ØªØ©","Ø·ÙŠÙ Ø§Ù„ØªØªØ¨Ø¹","ØªØ¯ÙÙ‚ Ø®ÙÙŠ","Ø¥Ø²Ø§Ø­Ø© Ù…Ù‚ØµÙˆØ¯Ø©",
]
_EN_VARIANTS = [
    "Shadow Variant","Evasive Pattern","Calm-Control Line","Tactical Deconstruction",
    "Counter-Maneuver","Silent Angle","Tracking Flux","Stealth Flow","Angle Shift",
]

def _generate_variant_label(base: str, lang: str, salt: int = 0) -> str:
    base = (base or "").strip(" -â€”:ØŒ")
    pool = _AR_VARIANTS if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else _EN_VARIANTS
    joiner = " â€” "
    idx = abs(hash(_normalize_ar(base) + str(salt))) % len(pool)
    return f"{base}{joiner}{pool[idx]}"

def _perturb_phrasing(rec: Dict[str, Any], lang: str) -> Dict[str, Any]:
    r = dict(rec)
    tacky_add_ar = " Ø±ÙƒÙ‘Ø² Ø¹Ù„Ù‰ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ø¨Ø¯Ù„ Ù…Ø·Ø§Ø±Ø¯Ø© Ø§Ù„Ø­Ø±ÙƒØ©. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù‚Ø±Ø§Ø± ÙŠØ¸Ù‡Ø± ÙØ¬Ø£Ø© Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ‡Ø¯Ø£ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹."
    tacky_add_en = " Emphasize reading angles over chasing motion; let decisions snap when the rhythm calms."
    if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
        r["what_it_looks_like"] = (r.get("what_it_looks_like","") + tacky_add_ar).strip()
        r["why_you"] = (r.get("why_you","") + " ØªØ­Ø¨ Ø§Ù„Ø§Ø®ØªØµØ§Ø± Ø§Ù„Ø°ÙƒÙŠ ÙˆØ¥Ø®ÙØ§Ø¡ Ù†ÙˆØ§ÙŠØ§Ùƒ Ø­ØªÙ‰ Ø§Ù„Ù„Ø­Ø¸Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.").strip()
    else:
        r["what_it_looks_like"] = (r.get("what_it_looks_like","") + tacky_add_en).strip()
        r["why_you"] = (r.get("why_you","") + " You like smart brevity and hiding intent until the decisive beat.").strip()
    return r

def _ensure_unique_labels_v_global(recs: List[Dict[str, Any]], lang: str, bl: dict) -> List[Dict[str, Any]]:
    used_now = set()
    out = []
    for r in recs:
        r = dict(r or {})
        label = r.get("sport_label") or ""
        if _is_forbidden_generic(label) or not label.strip():
            label = "ØªÙƒØªÙŠÙƒÙŠ ØªØ®ÙÙ‘ÙŠ" if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "Tactical Stealth"
            r["sport_label"] = label
        salt = 0
        while _bl_has(bl, label) or _canon_label(label) in used_now:
            label = _generate_variant_label(label, lang, salt=salt)
            r = _perturb_phrasing(r, lang)
            salt += 1
        used_now.add(_canon_label(label))
        r["sport_label"] = label
        out.append(r)
    return out

def _persist_blacklist(recs: List[Dict[str, Any]], bl: dict) -> None:
    def _write():
        for r in recs:
            lab = r.get("sport_label") or ""
            if lab.strip():
                _bl_add(bl, lab, alias=lab)
        _save_json_atomic(BL_PATH, bl)

    if FILELOCK_OK:
        try:
            with FileLock(str(BL_PATH) + ".lock", timeout=4):
                _write()
        except Timeout:
            _warn("blacklist lock timeout on write; writing without lock")
            _write()
    else:
        time.sleep(0.01 + random.random()*0.03)
        _write()

# ========= Prompt Builder / JSON parsing =========
def _style_seed(user_id: str, profile: Optional[Dict[str, Any]]) -> int:
    base = user_id or "anon"
    axes = profile.get("axes", {}) if isinstance(profile, dict) else {}
    s = f"{base}:{json.dumps(axes, sort_keys=True, ensure_ascii=False)}"
    h = hashlib.sha256(s.encode("utf-8")).hexdigest()
    return int(h[:8], 16)

def _answers_to_bullets(answers: Dict[str, Any]) -> str:
    lines: List[str] = []
    for k, v in (answers or {}).items():
        lines.append(f"- {k}: {_norm_answer_value(v)}")
    text = "\n".join(lines)
    if len(text) > MAX_PROMPT_CHARS//2:
        text = text[:MAX_PROMPT_CHARS//2] + "\n- â€¦"
    return text

def _strip_code_fence(s: str) -> str:
    if not s: return s
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json)?\s*", "", s)
        s = re.sub(r"\s*```$", "", s)
    return s

def _parse_llm_json(txt: str) -> Optional[dict]:
    if not txt: return None
    raw = _strip_code_fence(txt)
    try:
        return json.loads(raw)
    except Exception:
        pass
    m = re.search(r"\{.*\}", raw, re.DOTALL)
    if m:
        frag = m.group(0)
        try:
            return json.loads(frag)
        except Exception:
            if REC_REPAIR_ENABLED:
                frag2 = re.sub(r",(\s*[}\]])", r"\1", frag)
                try:
                    return json.loads(frag2)
                except Exception:
                    return None
    return None

def _too_generic(text: str, min_chars: int) -> bool:
    t = _normalize_ar(text or "")
    if len(t) < min_chars: return True
    return any(p in t for p in [_normalize_ar(x) for x in _AVOID_GENERIC])

def _has_sensory(text: str) -> bool:
    t = _normalize_ar(text or "")
    return any(_normalize_ar(s) in t for s in _SENSORY)

def _is_meaningful(rec: Dict[str, Any]) -> bool:
    lab = _norm_text(rec.get("sport_label","")).strip()
    looks = _norm_text(rec.get("what_it_looks_like","")).strip()
    return bool(lab and looks and len(_normalize_ar(looks)) >= 30)

def _compact_analysis_for_prompt(analysis: Dict[str, Any], profile: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    p_axes   = (profile or {}).get("axes", {})
    p_signals= (profile or {}).get("signals", [])
    hints    = (profile or {}).get("hints_for_prompt", "")
    out = {
        "silent_drivers": analysis.get("silent_drivers", []),
        "z_axes": analysis.get("z_axes", p_axes),
        "z_intent": analysis.get("z_intent", []),
        "encoded_profile": {"axes": p_axes, "signals": p_signals, "hints": _clip(str(hints), 300)},
    }
    blob = json.dumps(out, ensure_ascii=False)
    if len(blob) > MAX_PROMPT_CHARS // 2:
        out["encoded_profile"]["signals"] = out["encoded_profile"]["signals"][:10]
    return out

# ========= Core: KB-first then LLM fallback =========
def _kb_candidates(analysis: Dict[str, Any], answers: Dict[str, Any], lang: str) -> List[Dict[str, Any]]:
    profile = (analysis or {}).get("encoded_profile") or {}
    axes = (profile or {}).get("axes") or {}
    signals = _extract_signals(answers, lang)
    ids = KB.get("identities") or []
    ranked_from_kb: List[Dict[str, Any]] = []
    if ids:
        exp = _axes_expectations(axes or {}, lang)
        scored: List[Tuple[int, Dict[str, Any]]] = []
        for rec in ids:
            r = _sanitize_record(rec)
            blob = " ".join([_norm_text(r.get("what_it_looks_like","")),
                             _norm_text(r.get("why_you","")),
                             _norm_text(r.get("first_week",""))]).lower()
            hit = 0
            for words in exp.values():
                if words and any(w.lower() in blob for w in words): hit += 1
            if signals.get("precision") and ("precision" in blob or "Ø¯Ù‚Ù‡" in blob): hit += 1
            if signals.get("stealth") and ("stealth" in blob or "ØªØ®ÙÙŠ" in blob): hit += 1
            scored.append((hit, r))
        scored.sort(key=lambda x: x[0], reverse=True)
        ranked_from_kb = [s[1] for s in scored[:3]]
    return ranked_from_kb

def _llm_fallback(user_id: str, analysis: Dict[str, Any], answers: Dict[str, Any], lang: str) -> List[Dict[str, Any]]:
    if not LLM_CLIENT:
        return []
    profile = analysis.get("encoded_profile") or {}
    bullets = _answers_to_bullets(answers)
    compact = _compact_analysis_for_prompt(analysis, profile)

    sys_ar = (
        "Ø£Ù†Øª Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø±ÙŠØ§Ø¶ÙŠØ©. Ø£Ø¹Ø¯ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬ JSON. Ø§Ù…Ù†Ø¹ Ø°ÙƒØ± Ø§Ù„ÙˆÙ‚Øª/Ø§Ù„ØªÙƒÙ„ÙØ©/Ø§Ù„Ù…ÙƒØ§Ù†/Ø§Ù„Ø¹Ø¯Ø§Øª."
        " ÙŠØ¬Ø¨ Ø£Ù† ØªÙØ±Ø¬Ø¹ 3 Ø¹Ù†Ø§ØµØ± ØªØ­Øª Ø§Ù„Ù…ÙØªØ§Ø­ 'cards'ØŒ ÙƒÙ„ Ø¹Ù†ØµØ± ÙŠØ­ÙˆÙŠ: "
        "sport_label, what_it_looks_like, inner_sensation, why_you, first_week, progress_markers, win_condition, core_skills (list<=6), mode, variant_vr, variant_no_vr, difficulty(1-5)."
    )
    sys_en = (
        "You are a sport-identity recommender. Return JSON ONLY, no prose. Forbid time/cost/location/equipment mentions."
        " Return exactly 3 items under 'cards' with keys: "
        "sport_label, what_it_looks_like, inner_sensation, why_you, first_week, progress_markers, win_condition, core_skills (list<=6), mode, variant_vr, variant_no_vr, difficulty(1-5)."
    )
    system = sys_ar if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else sys_en
    user_blob = {
        "lang": lang,
        "answers_bullets": bullets,
        "profile_compact": compact,
        "constraints": {
            "no_time_cost_place_sets": True,
            "min_chars_each": _MIN_CHARS,
            "require_win": _REQUIRE_WIN,
            "min_core_skills": _MIN_CORE_SKILLS
        }
    }

    msgs = [
        {"role":"system","content":system},
        {"role":"user","content":json.dumps(user_blob, ensure_ascii=False)}
    ]
    temp_primary = 0.6 if not REC_FAST_MODE else 0.5
    temp_repair  = 0.35

    t0 = time.perf_counter()
    out_txt = ""
    try:
        out_txt = chat_once(
            client=LLM_CLIENT,
            model=CHAT_MODEL,
            messages=msgs,
            temperature=temp_primary,
            timeout_s=min(REC_BUDGET_S, 18.0),
        )
    except Exception as e:
        _warn(f"chat_once primary failed: {e}")
        try:
            out_txt = chat_once(
                client=LLM_CLIENT,
                model=CHAT_MODEL_FALLBACK or CHAT_MODEL,
                messages=msgs,
                temperature=temp_primary,
                timeout_s=min(REC_BUDGET_S, 18.0),
            )
        except Exception as e2:
            _err(f"chat_once fallback failed: {e2}")
            return []

    data = _parse_llm_json(out_txt or "")
    if not data and REC_REPAIR_ENABLED:
        _dbg("repair: forcing JSON only")
        msgs2 = [
            {"role":"system","content":system},
            {"role":"user","content":"Ø£Ø¹Ø¯ Ù†ÙØ³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬ JSON." if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
             else "Return the same answer as pure JSON only, with no prose outside JSON."}
        ]
        try:
            out_txt2 = chat_once(
                client=LLM_CLIENT,
                model=CHAT_MODEL_FALLBACK or CHAT_MODEL,
                messages=msgs + msgs2,
                temperature=temp_repair,
                timeout_s=max(4.0, min(REC_BUDGET_S - (time.perf_counter()-t0), 8.0)),
            )
            data = _parse_llm_json(out_txt2 or "")
        except Exception as e:
            _warn(f"repair round failed: {e}")
            data = None

    cards: List[Dict[str, Any]] = []
    if isinstance(data, dict) and isinstance(data.get("cards"), list):
        for item in data["cards"][:3]:
            if isinstance(item, dict):
                cards.append(_sanitize_record(item))
    return cards

# ========= Global ensure / rendering =========
def _fill_defaults_batch(cards: List[Dict[str, Any]], lang: str) -> List[Dict[str, Any]]:
    return [_fill_defaults(c, lang) for c in cards]

def _quality_filter(cards: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out = []
    for c in cards:
        if not _is_meaningful(c): 
            continue
        if _too_generic(c.get("what_it_looks_like",""), _MIN_CHARS):
            continue
        if _REQUIRE_WIN and not (c.get("win_condition") or "").strip():
            continue
        if len(c.get("core_skills") or []) < _MIN_CORE_SKILLS:
            continue
        # Ù„ØªØ´Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø³Ù‘ Ø§Ù„Ø¬Ø³Ø¯ÙŠ ÙØ¹Ù‘Ù„:
        # if not _has_sensory((c.get("inner_sensation","") + " " + c.get("what_it_looks_like","")).strip()):
        #     continue
        out.append(c)
    return out

def _brand_footer(lang: str) -> str:
    return "â€” Sports Sync" if lang != "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€” Sports Sync"

def _format_card(c: Dict[str, Any], lang: str) -> str:
    lines: List[str] = []
    lines.append(f"### {c.get('sport_label','')}")
    if c.get('what_it_looks_like'):
        lines.append(f"{c['what_it_looks_like']}")
    if c.get('inner_sensation'):
        lines.append(f"â€¢ Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ: {c['inner_sensation']}" if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else f"â€¢ Inner feel: {c['inner_sensation']}")
    if c.get('why_you'):
        lines.append(f"â€¢ Ù„Ù…Ø§Ø°Ø§ Ø£Ù†Øª: {c['why_you']}" if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else f"â€¢ Why you: {c['why_you']}")
    if c.get('first_week'):
        lines.append(f"â€¢ Ø£ÙˆÙ„ Ø£Ø³Ø¨ÙˆØ¹: {c['first_week']}" if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else f"â€¢ First week: {c['first_week']}")

    pm = c.get('progress_markers')
    if isinstance(pm, (list, tuple, set)):
        pm_list = _to_bullets(pm, 6)
        if pm_list:
            lines.append(("â€¢ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ‚Ø¯Ù…: " if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€¢ Progress markers: ")
                         + "ØŒ ".join(map(str, pm_list)))
    elif pm:
        lines.append(f"â€¢ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ‚Ø¯Ù…: {pm}" if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else f"â€¢ Progress markers: {pm}")

    if c.get('win_condition'):
        lines.append(f"â€¢ Ø´Ø±Ø· Ø§Ù„ÙÙˆØ²/Ø§Ù„Ù‡Ø¯Ù: {c['win_condition']}" if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else f"â€¢ Win condition: {c['win_condition']}")

    skills = _to_bullets(c.get('core_skills', []), 6)
    if skills:
        lines.append(("â€¢ Ù…Ù‡Ø§Ø±Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©: " if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€¢ Core skills: ")
                     + "ØŒ ".join(map(str, skills)))

    if c.get('mode'):
        lines.append(("â€¢ Ø§Ù„Ù†Ù…Ø·: " if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€¢ Mode: ") + str(c['mode']))
    if c.get('variant_vr'):
        lines.append(("â€¢ Ù†Ø³Ø®Ø© VR: " if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€¢ VR variant: ") + str(c['variant_vr']))
    if c.get('variant_no_vr'):
        lines.append(("â€¢ Ù†Ø³Ø®Ø© Ø¨Ù„Ø§ VR: " if lang=="Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "â€¢ No-VR variant: ") + str(c['variant_no_vr']))

    lines.append(_brand_footer(lang))
    return "\n".join(map(str, lines)).strip()

# ========= Public API =========
def generate_sport_recommendation(answers: Dict[str, Any], lang: str = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©") -> List[str]:
    """
    ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ù‘Ø¯Ø© Ù„Ù€ app.py: ØªØ±Ø¬Ø¹ 3 Ø¨Ø·Ø§Ù‚Ø§Øª Ù†ØµÙ‘ÙŠØ© (List[str]).
    - Evidence Gate â†’ followups Ø¥Ù† Ù„Ø²Ù…
    - KB-first
    - LLM fallback (Ø¥Ù† ØªÙˆÙØ± Ù…ÙØªØ§Ø­)
    - Sanitize + dedupe + blacklist global uniqueness
    """
    t0 = time.perf_counter()
    timings: Dict[str, int] = {}

    # 0) Evidence Gate
    eg_t0 = time.perf_counter()
    eg = _run_egate(answers, lang=lang)
    timings["egate_ms"] = int((time.perf_counter() - eg_t0) * 1000)
    if eg.get("status") != "pass":
        follow = _format_followup_card(eg.get("followup_questions", [])[:3], lang)
        _dbg(f"egate: {timings['egate_ms']}ms | blocked (need followups)")
        return [follow, "â€”", "â€”"]

    user_id = "web_user"

    # 0.1) Analysis (silent drivers + encoded profile + intent)
    ana_t0 = time.perf_counter()
    analysis = {
        "encoded_profile": _extract_profile(answers, lang) or {},
        "silent_drivers": analyze_silent_drivers(answers, lang=lang),
        "z_intent": _call_analyze_intent(answers, lang=lang)
    }
    timings["analysis_ms"] = int((time.perf_counter() - ana_t0) * 1000)

    # 1) KB candidates
    kb_t0 = time.perf_counter()
    cards_struct = _kb_candidates(analysis, answers, lang)
    timings["kb_ms"] = int((time.perf_counter() - kb_t0) * 1000)

    # 2) Fallback LLM Ù„Ùˆ Ù†Ø§Ù‚Øµ
    if len(cards_struct) < 3:
        llm_t0 = time.perf_counter()
        extra = _llm_fallback(user_id, analysis, answers, lang)
        timings["llm_ms"] = int((time.perf_counter() - llm_t0) * 1000)
        cards_struct += extra
    else:
        timings["llm_ms"] = 0

    if not cards_struct:
        cards_struct = [_fallback_identity(i, lang) for i in range(3)]

    # 3) Ø¬ÙˆØ¯Ù‡ + fill + dedupe
    post_t0 = time.perf_counter()
    cards_struct = _fill_defaults_batch(cards_struct, lang)
    cards_struct = _quality_filter(cards_struct)
    cards_struct = _hard_dedupe_and_fill(cards_struct, lang)
    timings["post_ms"] = int((time.perf_counter() - post_t0) * 1000)

    # 4) Global uniqueness via blacklist
    bl_t0 = time.perf_counter()
    bl = _load_blacklist()
    cards_struct = _ensure_unique_labels_v_global(cards_struct, lang, bl)
    _persist_blacklist(cards_struct, bl)
    timings["blacklist_ms"] = int((time.perf_counter() - bl_t0) * 1000)

    # 5) Scrub URLs (Ù„Ùˆ ÙÙŠÙ‡)
    scrub_t0 = time.perf_counter()
    cards_struct = [json.loads(scrub_unknown_urls(json.dumps(c, ensure_ascii=False), CFG)) if isinstance(c, dict) else c
                    for c in cards_struct]
    timings["scrub_ms"] = int((time.perf_counter() - scrub_t0) * 1000)

    # 6) Render â†’ List[str] (force-safe strings)
    render_t0 = time.perf_counter()
    rendered = [_format_card(c, lang) for c in cards_struct[:3]]
    rendered = [str(x) for x in rendered]
    timings["render_ms"] = int((time.perf_counter() - render_t0) * 1000)

    timings["total_ms"] = int((time.perf_counter() - t0) * 1000)
    _dbg(
        "egate: {eg}ms | analysis: {ana}ms | kb: {kb}ms | llm: {llm}ms | "
        "post: {post}ms | bl: {bl}ms | scrub: {scrub}ms | render: {render}ms | total: {tot}ms"
        .format(
            eg=timings.get("egate_ms", 0),
            ana=timings.get("analysis_ms", 0),
            kb=timings.get("kb_ms", 0),
            llm=timings.get("llm_ms", 0),
            post=timings.get("post_ms", 0),
            bl=timings.get("blacklist_ms", 0),
            scrub=timings.get("scrub_ms", 0),
            render=timings.get("render_ms", 0),
            tot=timings.get("total_ms", 0),
        )
    )

    # 7) ØªØ³Ø¬ÙŠÙ„ Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ù†ØªÙŠØ¬Ø© (Ù„Ù† ÙŠØªØ³Ø¨Ø¨ ÙÙŠ ÙƒØ±Ø§Ø´ Ù„Ùˆ Ø§Ù„Ù…ÙˆØ¯ÙŠÙˆÙ„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)
    try:
        from core.user_logger import log_recommendation_result
        log_recommendation_result(
            user_id=user_id,
            answers=answers,
            recs_text=rendered,
            timings=timings,
            lang=lang,
            model=str(CHAT_MODEL) if 'CHAT_MODEL' in globals() else None,
            app_version=(CFG or {}).get("app_version")
        )
    except Exception:
        pass

    return rendered
