#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SportSync AI - User-Friendly Chat Interface
===========================================
Triple Intelligence System with Streaming Display
"""

import streamlit as st
import json
import time
from typing import Generator
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from core.ai_orchestrator import generate_sport_recommendations

# Page config
st.set_page_config(
    page_title="SportSync AI - Ù…ÙƒØªØ´Ù Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©",
    page_icon="ğŸ¯",
    layout="wide"
)

# Custom CSS for better UX
st.markdown("""
<style>
    .stApp {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    .main-header {
        text-align: center;
        padding: 2rem;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        backdrop-filter: blur(10px);
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 15px;
        margin: 1rem 0;
        animation: fadeIn 0.5s;
    }
    .user-message {
        background: rgba(255, 255, 255, 0.9);
        margin-left: 20%;
    }
    .ai-message {
        background: rgba(103, 126, 234, 0.2);
        margin-right: 20%;
        color: white;
    }
    .layer-indicator {
        display: inline-block;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        font-weight: bold;
        margin: 0.2rem;
    }
    .layer-fast { background: #10b981; color: white; }
    .layer-reasoning { background: #f59e0b; color: white; }
    .layer-intelligence { background: #8b5cf6; color: white; }
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'pipeline_status' not in st.session_state:
    st.session_state.pipeline_status = None


def display_layer_status(layer: str, status: str):
    """Display layer processing status"""
    colors = {
        "fast": "ğŸš€",
        "reasoning": "ğŸ§ ", 
        "intelligence": "ğŸ¯"
    }
    icon = colors.get(layer, "âš™ï¸")
    
    if status == "processing":
        return f"{icon} **{layer.upper()}** ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†..."
    elif status == "complete":
        return f"âœ… **{layer.upper()}** Ø§ÙƒØªÙ…Ù„"
    elif status == "failed":
        return f"âŒ **{layer.upper()}** ÙØ´Ù„"


def stream_text(text: str, delay: float = 0.02) -> Generator[str, None, None]:
    """Stream text character by character for better UX"""
    for char in text:
        yield char
        time.sleep(delay)


def format_recommendation(rec: dict, index: int) -> str:
    """Format a single recommendation beautifully"""
    icons = ["ğŸŸ¢", "ğŸŒ¿", "ğŸ”®"]
    icon = icons[index] if index < 3 else "â­"
    
    markdown = f"""
### {icon} Ø§Ù„ØªÙˆØµÙŠØ© Ø±Ù‚Ù… {index + 1}: {rec.get('title', 'Ø±ÙŠØ§Ø¶Ø© Ù…Ø¨ØªÙƒØ±Ø©')}

**âœ¨ Ø§Ù„Ø¬ÙˆÙ‡Ø±:**
{rec.get('essence', 'ØªØ¬Ø±Ø¨Ø© ÙØ±ÙŠØ¯Ø©')}

**ğŸ’« Ø§Ù„ØªØ¬Ø±Ø¨Ø©:**
{rec.get('experience', 'ÙˆØµÙ Ø§Ù„ØªØ¬Ø±Ø¨Ø©')}

**ğŸ¯ Ù„Ù…Ø§Ø°Ø§ Ù…Ø«Ø§Ù„ÙŠØ© Ù„Ùƒ:**
"""
    
    for reason in rec.get('why_perfect', []):
        markdown += f"\n- {reason}"
    
    markdown += f"\n\n**ğŸš€ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ø£ÙˆÙ„:**\n{rec.get('first_week', 'Ø§Ø¨Ø¯Ø£ Ø¨Ø®Ø·ÙˆØ§Øª ØµØºÙŠØ±Ø©')}"
    
    markdown += "\n\n**âœ… Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ‚Ø¯Ù…:**"
    for sign in rec.get('signs_of_progress', []):
        markdown += f"\n- {sign}"
    
    return markdown


# Header
st.markdown("""
<div class="main-header">
    <h1 style="color: white; font-size: 3rem; margin: 0;">ğŸ§  Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ</h1>
    <p style="color: rgba(255,255,255,0.9); font-size: 1.2rem;">
        Fast â†’ Reasoning â†’ Intelligence | Ù„Ø§ ÙŠÙˆØ¬Ø¯ Fallback âœ—
    </p>
</div>
""", unsafe_allow_html=True)

# Sidebar with system info
with st.sidebar:
    st.markdown("### âš™ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
    
    st.markdown("""
    **Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«:**
    - ğŸš€ **Fast**: GPT-3.5-turbo (Ø§Ø³ØªØ®Ù„Ø§Øµ Ø³Ø±ÙŠØ¹)
    - ğŸ§  **Reasoning**: o1-mini (ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚)
    - ğŸ¯ **Intelligence**: GPT-4 (ØªÙˆØµÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©)
    """)
    
    st.markdown("---")
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []
        st.session_state.pipeline_status = None
        st.rerun()
    
    st.markdown("---")
    st.markdown("**ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:**")
    st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„: {len(st.session_state.messages)}")

# Chat interface
st.markdown("### ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")

# Display chat history
for message in st.session_state.messages:
    css_class = "user-message" if message["role"] == "user" else "ai-message"
    st.markdown(f"""
    <div class="chat-message {css_class}">
        <strong>{"ğŸ§‘ Ø£Ù†Øª" if message["role"] == "user" else "ğŸ¤– SportSync AI"}:</strong><br>
        {message["content"]}
    </div>
    """, unsafe_allow_html=True)

# User input
user_input = st.chat_input("Ø§ÙƒØªØ¨ ÙˆØµÙØ§Ù‹ Ù„Ù…Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ ÙÙŠ Ø±ÙŠØ§Ø¶Ø©...")

if user_input:
    # Add user message
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })
    
    # Display user message immediately
    st.markdown(f"""
    <div class="chat-message user-message">
        <strong>ğŸ§‘ Ø£Ù†Øª:</strong><br>
        {user_input}
    </div>
    """, unsafe_allow_html=True)
    
    # Create placeholder for AI response
    response_placeholder = st.empty()
    status_placeholder = st.empty()
    
    # Show processing status
    with status_placeholder.container():
        st.markdown("### âš™ï¸ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„...")
        
        layer_status_fast = st.empty()
        layer_status_reasoning = st.empty()
        layer_status_intelligence = st.empty()
        
        layer_status_fast.markdown(display_layer_status("fast", "processing"))
    
    # Generate recommendations
    try:
        result = generate_sport_recommendations(user_input, "ar")
        
        if not result["success"]:
            # Show error
            error_msg = "âŒ **ÙØ´Ù„ Ø§Ù„Ù†Ø¸Ø§Ù…!**\n\n"
            error_msg += "\n".join(f"- {err}" for err in result["errors"])
            
            response_placeholder.markdown(f"""
            <div class="chat-message ai-message">
                <strong>ğŸ¤– SportSync AI:</strong><br>
                {error_msg}
            </div>
            """, unsafe_allow_html=True)
            
            st.session_state.messages.append({
                "role": "assistant",
                "content": error_msg
            })
            
        else:
            # Update status for each layer
            layer_status_fast.markdown(display_layer_status("fast", "complete"))
            layer_status_reasoning.markdown(display_layer_status("reasoning", "processing"))
            time.sleep(0.5)
            
            layer_status_reasoning.markdown(display_layer_status("reasoning", "complete"))
            layer_status_intelligence.markdown(display_layer_status("intelligence", "processing"))
            time.sleep(0.5)
            
            layer_status_intelligence.markdown(display_layer_status("intelligence", "complete"))
            
            # Clear status, show recommendations
            status_placeholder.empty()
            
            # Parse recommendations
            try:
                recs_data = json.loads(result["final_recommendations"])
                recommendations = recs_data.get("recommendations", [])
                
                if not recommendations:
                    raise ValueError("No recommendations found")
                
                # Build response with streaming effect
                full_response = "âœ… **ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­!**\n\n"
                full_response += f"ğŸ“Š **Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬**: {result['total_tokens']} token\n\n"
                full_response += "---\n\n"
                
                for i, rec in enumerate(recommendations[:3]):
                    full_response += format_recommendation(rec, i)
                    full_response += "\n\n---\n\n"
                
                # Display with streaming effect
                response_text = ""
                response_container = response_placeholder.container()
                
                with response_container:
                    text_placeholder = st.empty()
                    
                    # Stream the text
                    for char in full_response:
                        response_text += char
                        text_placeholder.markdown(f"""
                        <div class="chat-message ai-message">
                            <strong>ğŸ¤– SportSync AI:</strong><br>
                            {response_text}
                        </div>
                        """, unsafe_allow_html=True)
                        time.sleep(0.01)  # Adjust speed here
                
                # Save to session
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                
            except json.JSONDecodeError:
                error_msg = "âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
                response_placeholder.markdown(f"""
                <div class="chat-message ai-message">
                    <strong>ğŸ¤– SportSync AI:</strong><br>
                    {error_msg}
                </div>
                """, unsafe_allow_html=True)
                
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })
    
    except Exception as e:
        error_msg = f"âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹:**\n\n{str(e)}"
        response_placeholder.markdown(f"""
        <div class="chat-message ai-message">
            <strong>ğŸ¤– SportSync AI:</strong><br>
            {error_msg}
        </div>
        """, unsafe_allow_html=True)
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": error_msg
        })

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: rgba(255,255,255,0.7); padding: 1rem;">
    <p>SportSync AI - Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„</p>
    <p style="font-size: 0.8rem;">Powered by OpenAI GPT-3.5, o1-mini, and GPT-4</p>
</div>
""", unsafe_allow_html=True)


# Quick start examples
if len(st.session_state.messages) == 0:
    st.markdown("### ğŸ’¡ Ø¬Ø±Ø¨ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù…Ø«Ù„Ø©:")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ§˜ Ø£Ø±ÙŠØ¯ Ø´ÙŠØ¦Ø§Ù‹ Ù‡Ø§Ø¯Ø¦Ø§Ù‹"):
            st.session_state.messages.append({
                "role": "user",
                "content": "Ø£Ø¨Ø­Ø« Ø¹Ù† Ø±ÙŠØ§Ø¶Ø© Ù‡Ø§Ø¯Ø¦Ø© ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ÙƒÙŠØ² ÙˆØ§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡"
            })
            st.rerun()
    
    with col2:
        if st.button("ğŸ¯ Ø£Ø­Ø¨ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ù‚Ù„ÙŠØ©"):
            st.session_state.messages.append({
                "role": "user",
                "content": "Ø£Ø±ÙŠØ¯ Ø±ÙŠØ§Ø¶Ø© ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙˆØ§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ø¨Ø¯Ù†ÙŠØ©"
            })
            st.rerun()
    
    with col3:
        if st.button("ğŸ‘¥ Ø£ÙØ¶Ù„ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©"):
            st.session_state.messages.append({
                "role": "user",
                "content": "Ø£Ø¨Ø­Ø« Ø¹Ù† Ø±ÙŠØ§Ø¶Ø© Ø¬Ù…Ø§Ø¹ÙŠØ© Ù…Ù…ØªØ¹Ø© ØªØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø¹Ù„Ù‰ Ø¨Ù†Ø§Ø¡ ØµØ¯Ø§Ù‚Ø§Øª"
            })
            st.rerun()
