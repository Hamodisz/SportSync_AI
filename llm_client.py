# -*- coding: utf-8 -*-
"""
LLM Client (Unified, Singleton, Rotating Models)
================================================
Ù…ÙˆØ­Ù‘ÙØ¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ù…Ø§Ø°Ø¬ OpenAI/Groq/OpenRouter (ÙˆØ§Ø¬Ù‡Ø© OpenAI Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø©).

Ø§Ù„Ù…Ø²Ø§ÙŠØ§:
- ØªØ­Ù…ÙŠÙ„ Ø¢Ù…Ù† Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¨ÙŠØ¦Ø©: .env Ø«Ù… st.secrets (Ø¨Ø¯ÙˆÙ† Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ env Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯)
- Ø¶Ø¨Ø· base_url ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ (Groq/OpenRouter) + ØªØ±ÙˆÙŠØ³Ø§Øª OpenRouter
- Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø¨Ø³Ù„Ø³Ù„Ø© (comma/newline/semicolon) Ù…Ø¹ ØªØ¯ÙˆÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠ + remap Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
- chat_once Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø°ÙƒÙŠØ© ÙˆØ¥Ø²Ø§Ù„Ø© penalties Ø¹Ù†Ø¯ Ø£Ø®Ø·Ø§Ø¡ 400
- Singleton + Lock Ù„ØªÙØ§Ø¯ÙŠ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªÙ‡ÙŠØ¦Ø© (Ù…Ø¹ ÙƒØ§Ø´ Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù€ Streamlit)
- Ù„ÙˆØ¬ Ù†Ø¸ÙŠÙ ÙŠÙØ·Ø¨Ø¹ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ ØªÙØ¹ÙŠÙ„ LLM_INIT_LOG=1

Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµØ¯ÙŠØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
- make_llm_client()                    â† ÙŠØ¨Ù†ÙŠ Ø¹Ù…ÙŠÙ„ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ Ø¹Ø¨Ø± Singleton)
- make_llm_client_singleton()          â† ÙŠØ±Ø¬Ø¹ Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù€ Singleton Ù…Ø¨Ø§Ø´Ø±Ø©
- pick_models()                        â† ÙŠØ¹ÙŠØ¯ (main_chain, fallback)
- get_models_cached()                  â† ÙŠØ¹ÙŠØ¯ Ù†ÙØ³ (main_chain, fallback) Ù…Ø¹ ÙƒØ§Ø´
- chat_once(client, messages, model, â€¦)â† Ù…ÙƒØ§Ù„Ù…Ø© Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ ØªØ¯ÙˆÙŠØ± Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
- get_client_and_models()              â† (client, main_chain, fallback)
- get_streamlit_client()               â† Ø¹Ù…ÙŠÙ„ Ù…Ø¹ ÙƒØ§Ø´ Streamlit (Ø¥Ù† ÙˆÙØ¬Ø¯Øª Ù…ÙƒØªØ¨Ø© streamlit)

Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
- Ø¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ Ù…ÙØªØ§Ø­ API ÙØ§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø³ØªØ±ÙØ¹ RuntimeErrorØŒ Ù„ÙƒÙ† Ø¨Ø¥Ù…ÙƒØ§Ù†
  Ø¨Ù‚ÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ÙˆØ¶Ø¹ KB-only Ø¨Ø¹Ø¯ Ø£Ù† ØªØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø±Ø¬Ø§Ø¹ None Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©.
"""

from __future__ import annotations

import os
import re
import time
import random
import threading
from typing import List, Dict, Optional, Any, Tuple
from pathlib import Path

# =========================
# Utilities & Logging
# =========================

def _log(msg: str) -> None:
    # Ù„ÙˆØ¬ Ø¯Ø§Ø¦Ù… Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
    print(f"[LLM_CLIENT] {msg}")

def _maybe_log_once(msg: str) -> None:
    # Ù„ÙˆØ¬ Ø§Ø®ØªÙŠØ§Ø±ÙŠ (Ù…Ø±Ù‘Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ø§Ø¯Ø©Ù‹) Ø¨ØªÙØ¹ÙŠÙ„ LLM_INIT_LOG=1
    if os.getenv("LLM_INIT_LOG", "0") == "1":
        print(msg)

# =========================
# Bootstrap Environment
# =========================

def _bootstrap_env() -> None:
    """Ø­Ù…Ù‘Ù„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ù† .env Ùˆ st.secrets Ø¨Ø¯ÙˆÙ† Ù…Ø§ ØªÙƒØ³Ù‘Ø± Ø£ÙŠ Ù‚ÙŠÙ… Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ env."""
    # 1) .env (Ù…Ø­Ù„ÙŠ/ÙƒÙˆØ¯Ø³Ø¨ÙŠØ³/Render)
    try:
        from dotenv import load_dotenv  # type: ignore
        # Ø¬Ø±Ù‘Ø¨ Ø¬Ø°ÙˆØ± Ù…Ø­ØªÙ…Ù„Ø©
        roots = [Path.cwd(), Path(__file__).resolve().parent.parent, Path(__file__).resolve().parent]
        tried = set()
        for root in roots:
            env_path = (root / ".env").resolve()
            if env_path in tried:
                continue
            tried.add(env_path)
            if env_path.exists():
                load_dotenv(env_path, override=False)
                _maybe_log_once(f"[dotenv] loaded: {env_path}")
                break
    except Exception:
        pass

    # 2) st.secrets (Streamlit) â€” merge non-overriding
    try:
        import streamlit as st  # type: ignore
        secrets = dict(getattr(st, "secrets", {})) or {}
        def _pull(k: str):
            v = secrets.get(k)
            if v and not os.getenv(k):
                os.environ[k] = str(v)
        for k in (
            "GROQ_API_KEY", "OPENAI_API_KEY", "OPENROUTER_API_KEY",
            "OPENAI_BASE_URL", "OPENROUTER_BASE_URL", "OPENAI_ORG",
            "OPENROUTER_REFERRER", "OPENROUTER_APP_TITLE",
            "CHAT_MODEL", "CHAT_MODEL_FALLBACK",
            "LLM_SEED",
        ):
            _pull(k)
        if secrets:
            _maybe_log_once("[secrets] streamlit.secrets merged into env (non-overriding).")
    except Exception:
        pass

_bootstrap_env()

# =========================
# OpenAI-compatible client
# =========================
try:
    # openai>=1.6,<2 Ø§Ù„Ù…Ø¹ÙŠØ§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯ (OpenAI class)
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

# =========================
# Model Name Remapping
# =========================

_MODEL_REMAP: Dict[str, str] = {
    # Ø£Ù…Ø«Ù„Ø© Groq Ù‚Ø¯ÙŠÙ…Ø© â†’ Ø¨Ø¯Ø§Ø¦Ù„ Ø­ÙŠÙ‘Ø©
    "llama3-70b-8192":            "llama-3.1-70b",
    "llama3-8b-8192":             "llama-3.1-8b-instant",
    "llama3-70b":                 "llama-3.1-70b",
    "llama3-8b":                  "llama-3.1-8b-instant",
    "llama-3.2-90b-text-preview": "llama-3.1-70b",
    "llama-3.1-70b-versatile":    "llama-3.1-70b",  # versatile ØªÙ… Ø¥ÙŠÙ‚Ø§ÙÙ‡
    # Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ OpenRouter:
    "meta-llama/llama-3.1-70b-instruct:free": "meta-llama/llama-3.1-70b-instruct",
    "mistralai/mixtral-8x7b:free":            "mistralai/mixtral-8x7b",
}

def _remap_model(name: str) -> str:
    name = (name or "").strip()
    return _MODEL_REMAP.get(name, name)

def _split_models_csv(s: str) -> List[str]:
    """
    ÙŠÙØµÙ„ Ø³Ù„Ø³Ù„Ø© Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (comma/newline/semicolon) ÙˆÙŠØ´ÙŠÙ„:
    - Ø§Ù„ÙØ±Ø§ØºØ§Øª
    - Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨)
    - Ø£ÙŠ Ø¨Ø§Ø¯Ø¦Ø© Ø¹Ù„Ù‰ Ø´ÙƒÙ„ key=value (Ù†Ø£Ø®Ø° Ù…Ø§ Ø¨Ø¹Ø¯ '=')
    """
    seen, out = set(), []
    for part in re.split(r"[,\n;]+", s or ""):
        tok = part.strip()
        if not tok:
            continue
        if "=" in tok:  # ÙŠØµÙ„Ù‘Ø­ Ø£Ø®Ø·Ø§Ø¡ Ù…Ø«Ù„: CHAT_MODEL=llama-3.1-70b
            tok = tok.split("=", 1)[-1].strip()
        if tok and tok not in seen:
            out.append(tok)
            seen.add(tok)
    return out

# =========================
# Singleton State
# =========================

__LLM_CLIENT_SINGLETON: Optional[OpenAI] = None
__LLM_CLIENT_LOCK = threading.Lock()
__MODELS_CACHE: Optional[Tuple[str, str]] = None

# =========================
# Client Builder (single source)
# =========================

def _build_client() -> Optional[OpenAI]:
    """
    ÙŠØ¨Ù†ÙŠ Ø¹Ù…ÙŠÙ„ OpenAI-compatible.
    ÙŠØ±Ø¬Ù‘Ø¹ None Ù„Ùˆ Ù…Ø§ ÙÙŠÙ‡ Ù…ÙØªØ§Ø­ â€” Ø¹Ø´Ø§Ù† ØªØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ Ù…Ø¹ KB-only.
    """
    key = os.getenv("GROQ_API_KEY") or os.getenv("OPENAI_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    if not key:
        _maybe_log_once("NO API KEY (GROQ_API_KEY/OPENAI_API_KEY/OPENROUTER_API_KEY). KB-only mode.")
        return None

    base = os.getenv("OPENAI_BASE_URL") or os.getenv("OPENROUTER_BASE_URL") or ""
    is_groq = bool(os.getenv("GROQ_API_KEY")) or base.startswith("https://api.groq.com")
    is_openrouter = bool(os.getenv("OPENROUTER_API_KEY")) or base.startswith("https://openrouter.ai")

    # Ø¶Ø¨Ø· base Ù„Ù„Ù€ compat providers
    if not base and is_groq:
        base = "https://api.groq.com/openai/v1"
    if not base and is_openrouter:
        base = "https://openrouter.ai/api/v1"

    org = os.getenv("OPENAI_ORG") or None

    # ØªØ±ÙˆÙŠØ³Ø§Øª OpenRouter
    default_headers: Dict[str, str] = {}
    if is_openrouter:
        ref = os.getenv("OPENROUTER_REFERRER")
        title = os.getenv("OPENROUTER_APP_TITLE")
        if ref:   default_headers["HTTP-Referer"] = ref  # type: ignore
        if title: default_headers["X-Title"] = title     # type: ignore

    kw: Dict[str, Any] = {"api_key": key}
    if base: kw["base_url"] = base
    if org:  kw["organization"] = org
    if default_headers: kw["default_headers"] = default_headers

    if OpenAI is None:
        return None

    try:
        client = OpenAI(**kw)
        provider = "Groq" if is_groq else ("OpenRouter" if is_openrouter else "OpenAI")
        _maybe_log_once(f"INIT OK | base_url={base or 'default'} | org={bool(org)} | provider={provider}")
        return client
    except Exception as e:
        _maybe_log_once(f"INIT FAILED: {e!r} (base_url={base or '(none)'})")
        return None

def make_llm_client() -> Optional[OpenAI]:
    """ÙˆØ§Ø¬Ù‡Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚ â€” ØªØ¨Ù†ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ù„ÙƒÙ† ÙØ¹Ù„ÙŠÙ‹Ø§ Ø³Ù†Ø¹ÙŠØ¯ Singleton)."""
    return make_llm_client_singleton()

def make_llm_client_singleton() -> Optional[OpenAI]:
    """ÙŠØ¹ÙŠØ¯ Ø¹Ù…ÙŠÙ„ Singleton (ÙŠØ¨Ù†ÙŠ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)."""
    global __LLM_CLIENT_SINGLETON, __MODELS_CACHE
    if __LLM_CLIENT_SINGLETON is not None:
        return __LLM_CLIENT_SINGLETON
    with __LLM_CLIENT_LOCK:
        if __LLM_CLIENT_SINGLETON is None:
            __LLM_CLIENT_SINGLETON = _build_client()
            __MODELS_CACHE = pick_models()
            if __LLM_CLIENT_SINGLETON is not None:
                _maybe_log_once("[LLM_CLIENT] INIT OK (singleton)")
                _maybe_log_once(f"[LLM_CLIENT] MODELS -> main={__MODELS_CACHE[0]}, fb={__MODELS_CACHE[1]}")
    return __LLM_CLIENT_SINGLETON

def get_models_cached() -> Tuple[str, str]:
    """ÙŠØ¹ÙŠØ¯ (main_chain, fallback) Ù…Ø¹ ÙƒØ§Ø´ Ø¯Ø§Ø®Ù„ÙŠ."""
    global __MODELS_CACHE
    if __MODELS_CACHE is None:
        _ = make_llm_client_singleton()
    # Ø¶Ù…Ø§Ù†Ø§Øª
    if not __MODELS_CACHE:
        __MODELS_CACHE = pick_models()
    return __MODELS_CACHE

def get_client_and_models() -> Tuple[Optional[OpenAI], str, str]:
    """Ø±Ø§Ø­Ø©: ÙŠØ±Ø¬Ù‘Ø¹ (client, main_chain, fallback)."""
    c = make_llm_client_singleton()
    main, fb = get_models_cached()
    return c, main, fb

# =========================
# Model Picking
# =========================

def pick_models() -> Tuple[str, str]:
    """
    ÙŠØ­Ø¯Ø¯ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø²ÙˆØ¯.
    Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ‚Ø¯Ø± ØªØ­Ø· Ø£ÙƒØ«Ø± Ù…Ù† Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ CHAT_MODEL Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„/Ø£Ø³Ø·Ø±/ÙÙˆØ§ØµÙ„ Ù…Ù†Ù‚ÙˆØ·Ø©.
    """
    using_groq = bool(
        os.getenv("GROQ_API_KEY")
        or str(os.getenv("OPENAI_BASE_URL", "")).startswith("https://api.groq.com")
    )
    using_openrouter = bool(
        os.getenv("OPENROUTER_API_KEY")
        or str(os.getenv("OPENROUTER_BASE_URL", "")).startswith("https://openrouter.ai")
    )

    if using_groq:
        chain_default = ",".join([
            "llama-3.1-70b",
            "llama-3.1-8b-instant",
            "mixtral-8x7b-32768",
            "gemma2-9b-it",
        ])
        fb_default = "llama-3.1-8b-instant"
    elif using_openrouter:
        # Ø£Ù…Ø«Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ OpenRouter (Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø§ Ù„Ø¯ÙŠÙƒ ÙØ¹Ù„ÙŠÙ‹Ø§)
        chain_default = ",".join([
            "meta-llama/llama-3.3-70b-instruct:free",
            "mistralai/mixtral-8x7b:free",
        ])
        fb_default = "mistralai/mixtral-8x7b:free"
    else:
        chain_default = ",".join(["gpt-4o", "gpt-4o-mini", "gpt-4.1-mini"])
        fb_default = "gpt-4o-mini"

    raw_main = (os.getenv("CHAT_MODEL", chain_default) or "").strip()
    raw_fb   = (os.getenv("CHAT_MODEL_FALLBACK", fb_default) or "").strip()

    # Ù†Ø¸Ù‘Ù Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø«Ù… remap Ù„ÙƒÙ„ Ø¹Ù†ØµØ±
    chain = [_remap_model(x) for x in _split_models_csv(raw_main)]
    fb = _remap_model((_split_models_csv(raw_fb)[:1] or [fb_default])[0])

    # Ø¶Ù…Ù‘Ù† fallback Ø¶Ù…Ù† Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯
    if fb and fb not in chain:
        chain.append(fb)

    main_chain = ",".join(chain) if chain else fb
    _maybe_log_once(f"MODELS -> main={main_chain}, fb={fb}")
    return (main_chain, fb)

# =========================
# Chat (with rotation)
# =========================

def chat_once(
    client: Optional[OpenAI],
    messages: List[Dict[str, str]],
    model: str,
    *,
    temperature: float = 0.5,
    max_tokens: int = 800,
    top_p: float = 0.9,
    presence_penalty: float = 0.15,
    frequency_penalty: float = 0.1,
    timeout_s: float = 20.0,
    retries: int = 2,
    seed: Optional[int] = None,
) -> str:
    """
    Ù…ÙƒØ§Ù„Ù…Ø© Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ø¯ÙˆØ±Ø§Ù† ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Ù„Ùˆ ØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§).
    - Ø¹Ù†Ø¯ 400/Ø±ÙØ¶ Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª: Ù†Ø­Ø§ÙˆÙ„ Ù†ÙØ³ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¥Ø²Ø§Ù„Ø© penalties ÙˆØ¶Ø¨Ø· top_p=1
    - Ø¹Ù†Ø¯ "model not found"/"decommissioned"/404: Ù†Ù†ØªÙ‚Ù„ Ù„Ù„Ù…Ø±Ø´Ø­ Ø§Ù„ØªØ§Ù„ÙŠ
    - Ø¹Ù†Ø¯ ÙØ´Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©: Ø±Ø³Ø§Ø¦Ù„ ÙˆØ§Ø¶Ø­Ø© Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ù…ÙÙŠØ¯Ø©
    """
    if client is None:
        error_msg = (
            "\nâŒ Ø®Ø·Ø£: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙŠÙ„ LLM - Ø§Ù„Ù…ÙØªØ§Ø­ ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ·!\n"
            "\nğŸ“‹ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ù„:\n"
            "1. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ API Ù…Ø¬Ø§Ù†ÙŠ Ù…Ù†:\n"
            "   â€¢ Groq (Ù…Ø¬Ø§Ù†ÙŠ): https://console.groq.com/keys\n"
            "   â€¢ Ø£Ùˆ OpenAI (Ù…Ø¯ÙÙˆØ¹): https://platform.openai.com/api-keys\n"
            "\n2. Ø¶Ø¹Ù‡ ÙÙŠ Ù…Ù„Ù .env:\n"
            "   GROQ_API_KEY=gsk_your_key_here\n"
            "   Ø£Ùˆ\n"
            "   OPENAI_API_KEY=sk-proj-your_key_here\n"
            "\n3. Ø§Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙˆØ£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚\n"
            "\nğŸ’¡ Ø¨Ø¯ÙŠÙ„: ÙØ¹Ù‘Ù„ ÙˆØ¶Ø¹ KB ÙÙ‚Ø· ÙÙŠ .env:\n"
            "   ENABLE_KB_FALLBACK=true\n"
            "\n"
            "ERROR: No LLM client - API key not configured!\n"
            "\nSOLUTIONS:\n"
            "1. Get FREE API key from:\n"
            "   â€¢ Groq: https://console.groq.com/keys\n"
            "   â€¢ Or OpenAI: https://platform.openai.com/api-keys\n"
            "\n2. Add to .env file:\n"
            "   GROQ_API_KEY=gsk_your_key_here\n"
            "   or\n"
            "   OPENAI_API_KEY=sk-proj-your_key_here\n"
            "\n3. Save and restart app\n"
            "\nALTERNATIVE: Enable KB-only mode in .env:\n"
            "   ENABLE_KB_FALLBACK=true\n"
        )
        raise RuntimeError(error_msg)

    # Ø§Ø¨Ù†Ù Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
    candidate_models = [_remap_model(x) for x in _split_models_csv(model)]
    if not candidate_models:
        raise RuntimeError("Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ ØµØ§Ù„Ø­ ÙÙŠ 'model'.")

    if seed is None:
        env_seed = os.getenv("LLM_SEED")
        if env_seed:
            try:
                seed = int(env_seed)
            except Exception:
                seed = None

    last_err: Optional[Exception] = None
    for midx, model_id in enumerate(candidate_models):
        _maybe_log_once(f"trying model[{midx}]: {model_id}")
        for attempt in range(retries + 1):
            try:
                client_t = client.with_options(timeout=timeout_s)
                kwargs: Dict[str, Any] = dict(
                    model=model_id,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    presence_penalty=presence_penalty,
                    frequency_penalty=frequency_penalty,
                    max_tokens=max_tokens,
                )
                if seed is not None:
                    kwargs["seed"] = seed  # Ù‚Ø¯ ÙŠÙØªØ¬Ø§Ù‡Ù„ Ù„Ø¯Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†

                # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
                try:
                    resp = client_t.chat.completions.create(**kwargs)
                except Exception as inner_e:
                    msg = str(inner_e)
                    low = msg.lower()

                    # Ø±ÙØ¶ Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª â€” Ø¬Ø±Ù‘Ø¨ Ù†ÙØ³ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¯ÙˆÙ† penalties ÙˆØ¨Ù€ top_p=1
                    bad_params = ("400" in msg) and (("bad request" in low) or ("invalid" in low))
                    not_model_issue = not any(k in low for k in [
                        "model_decommissioned", "no such model", "model not found",
                        "unknown model", "does not exist", "404"
                    ])
                    if bad_params and not_model_issue:
                        _maybe_log_once("retrying without penalties/top_p due to provider param rejectionâ€¦")
                        kwargs.pop("presence_penalty", None)
                        kwargs.pop("frequency_penalty", None)
                        kwargs["top_p"] = 1.0
                        resp = client_t.chat.completions.create(**kwargs)
                    else:
                        raise

                content = (resp.choices[0].message.content if resp and getattr(resp, "choices", None) else "") or ""
                return content.strip()

            except Exception as e:
                last_err = e
                emsg = str(e).lower()
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©
                if any(k in emsg for k in ["authentication", "unauthorized", "invalid api key", "incorrect api key", "401"]):
                    error_msg = (
                        "\nâŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©: Ù…ÙØªØ§Ø­ API ØºÙŠØ± ØµØ­ÙŠØ­ Ø£Ùˆ Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©!\n"
                        "\nğŸ“‹ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø­Ù„:\n"
                        "1. ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API ÙÙŠ Ù…Ù„Ù .env\n"
                        "2. Ù…ÙØ§ØªÙŠØ­ Groq ØªØ¨Ø¯Ø£ Ø¨Ù€: gsk_\n"
                        "3. Ù…ÙØ§ØªÙŠØ­ OpenAI ØªØ¨Ø¯Ø£ Ø¨Ù€: sk-proj- Ø£Ùˆ sk-\n"
                        "4. Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³Ø§ÙØ§Øª Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Ø­ÙˆÙ„ Ø§Ù„Ù…ÙØªØ§Ø­\n"
                        "\nğŸ”‘ Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Ø¬Ø¯ÙŠØ¯:\n"
                        "   â€¢ Groq (Ù…Ø¬Ø§Ù†ÙŠ): https://console.groq.com/keys\n"
                        "   â€¢ OpenAI (Ù…Ø¯ÙÙˆØ¹): https://platform.openai.com/api-keys\n"
                        "\n"
                        "ERROR: Authentication failed - Invalid or expired API key!\n"
                        "\nSOLUTIONS:\n"
                        "1. Check your API key in .env file\n"
                        "2. Groq keys start with: gsk_\n"
                        "3. OpenAI keys start with: sk-proj- or sk-\n"
                        "4. No spaces or quotes around the key\n"
                        "\nGET NEW KEY:\n"
                        "   â€¢ Groq (free): https://console.groq.com/keys\n"
                        "   â€¢ OpenAI: https://platform.openai.com/api-keys\n"
                    )
                    _log(error_msg)
                    raise RuntimeError(error_msg) from e
                
                _maybe_log_once(f"chat attempt#{attempt} failed on {model_id}: {e!r}")

                # Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…ØªÙˆÙ‚Ù/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ â†’ Ø§Ù†ØªÙ‚Ù„ Ù„Ù„Ù…Ø±Ø´Ø­ Ø§Ù„ØªØ§Ù„ÙŠ ÙÙˆØ±Ù‹Ø§
                if any(k in emsg for k in [
                    "model_decommissioned", "no such model", "model not found", "unknown model",
                    "does not exist", "404"
                ]):
                    _maybe_log_once(f"model unavailable: {model_id} â†’ try next candidate")
                    break  # Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ

                # ÙˆØ¥Ù„Ø§ â†’ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ù†ÙØ³ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (exponential backoff)
                if attempt < retries:
                    time.sleep(0.6 * (2 ** attempt) + random.random() * 0.2)
                else:
                    break  # Ø®Ù„ØµØª Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„

    raise RuntimeError(f"ÙØ´Ù„ chat_once Ø¨Ø¹Ø¯ ØªØ¯ÙˆÙŠØ± ÙƒÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª: {last_err}")

# =========================
# Optional: Streamlit Cache
# =========================

def get_streamlit_client() -> Optional[OpenAI]:
    """
    Ø¥Ù† ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… StreamlitØŒ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¹ÙŠØ¯ Ø¹Ù…ÙŠÙ„Ù‹Ø§ Ø¨ÙƒØ§Ø´ resource.
    ØªØ¹Ù…Ù„ Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ØªØªÙˆÙØ± streamlit (ØªØ±Ø¬Ø¹ Singleton Ø¹Ø§Ø¯ÙŠ).
    """
    try:
        import streamlit as st  # type: ignore

        @st.cache_resource(show_spinner=False)
        def _client_cached():
            return make_llm_client_singleton()

        return _client_cached()
    except Exception:
        # fallback: Singleton Ø§Ù„Ø¹Ø§Ø¯ÙŠ
        return make_llm_client_singleton()

# =========================
# __all__
# =========================

__all__ = [
    "make_llm_client",
    "make_llm_client_singleton",
    "pick_models",
    "get_models_cached",
    "chat_once",
    "get_client_and_models",
    "get_streamlit_client",
]
